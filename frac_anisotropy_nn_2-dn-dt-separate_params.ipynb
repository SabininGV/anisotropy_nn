{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для воспроизводимости эксперимента:\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesample = 0.004\n",
    "start_time = 1.0\n",
    "end_time = 2.0\n",
    "n_receivers = 80\n",
    "n_timesamples = int((end_time - start_time)/timesample)\n",
    "\n",
    "data = np.empty([1,2,n_receivers,n_timesamples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 1100\n",
      "200 / 1100\n",
      "300 / 1100\n",
      "400 / 1100\n",
      "500 / 1100\n",
      "600 / 1100\n",
      "700 / 1100\n",
      "800 / 1100\n",
      "900 / 1100\n",
      "1000 / 1100\n",
      "1100 / 1100\n"
     ]
    }
   ],
   "source": [
    "n_models = 1100\n",
    "\n",
    "for i in range(n_models):\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(i+1,'/',n_models)\n",
    "    data_Z = pd.read_csv('csv_models_2frac_Z\\\\model_{}.csv'.format(i+1))\n",
    "    model_gather_Z = np.array(data_Z[data_Z.columns[int(start_time/timesample):int(end_time/timesample)]])\n",
    "    \n",
    "    data_X = pd.read_csv('csv_models_2frac_X\\\\model_{}.csv'.format(i+1))\n",
    "    model_gather_X = np.array(data_X[data_X.columns[int(start_time/timesample):int(end_time/timesample)]])\n",
    "    \n",
    "    model_gather = np.vstack(([model_gather_Z],[model_gather_X]))\n",
    "    model_gather = np.reshape(model_gather,(1,2,n_receivers,n_timesamples))\n",
    "    \n",
    "    data = np.vstack((data,model_gather))\n",
    "    \n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 2, 80, 250)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# центрирование и нормализация данных\n",
    "for i in range(n_models):\n",
    "    for j in range(2):\n",
    "        data[i,j] = scale(data[i,j], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_params = pd.read_csv('frac_params_2frac_sys.csv')\n",
    "frac_params['dt1'] *= 2.5\n",
    "frac_params['e1'] *= 6.3\n",
    "frac_params['dt2'] *= 2.5\n",
    "frac_params['e2'] *= 6.3\n",
    "frac_params = np.array(frac_params[['dn1','dt1','e1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(data[:800])\n",
    "X_validation = torch.Tensor(data[800:1100])\n",
    "\n",
    "Y_train = torch.Tensor(frac_params[:800])\n",
    "Y_validation = torch.Tensor(frac_params[800:1100])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class FracAnisotropyNet(torch.nn.Module):\n",
    "    def __init__(self,n1,n2,n3):\n",
    "        super(FracAnisotropyNet,self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=2, out_channels=n1, kernel_size=(2,4), padding=0, stride=2)\n",
    "        self.ac1 = torch.nn.ReLU()\n",
    "        self.bn1 = torch.nn.BatchNorm2d(num_features=n1)\n",
    "        #self.do1 = torch.nn.Dropout(p=prob)\n",
    "        #self.pool1 = torch.nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=n1, out_channels=n2, kernel_size=3, padding=0, stride=2)\n",
    "        self.ac2 = torch.nn.ReLU()\n",
    "        self.bn2 = torch.nn.BatchNorm2d(num_features=n2)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=n2, out_channels=n3, kernel_size=(2,3), padding=1, stride=(1,2))\n",
    "        self.ac3 = torch.nn.ReLU()\n",
    "        self.bn3 = torch.nn.BatchNorm2d(num_features=n3)\n",
    "        self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(10*15*n3,50)\n",
    "        self.ac1_ = torch.nn.Tanh()\n",
    "        #self.bn1_ = torch.nn.BatchNorm1d(num_features=80)\n",
    "        self.fc2 = torch.nn.Linear(50,20)\n",
    "        self.ac2_ = torch.nn.Tanh()\n",
    "        \n",
    "        self.out = torch.nn.Linear(20,3) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.ac1(x)\n",
    "        x = self.bn1(x)\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "            \n",
    "        x = self.conv2(x)\n",
    "        x = self.ac2(x)\n",
    "        x = self.bn2(x)\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "            \n",
    "        x = self.conv3(x)\n",
    "        x = self.ac3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool3(x)\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "        \n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.ac1_(x)\n",
    "        #x = self.bn1_(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ac2_(x)\n",
    "            \n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# изменил кол-во нейронов в полносвязных слоях\n",
    "class FracAnisotropyNet(torch.nn.Module):\n",
    "    def __init__(self,n1,n2,n3,n4,prob=0.0):\n",
    "        super(FracAnisotropyNet,self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=2, out_channels=n1, kernel_size=(2,4), padding=0, stride=2)\n",
    "        self.ac1 = torch.nn.ReLU()\n",
    "        self.bn1 = torch.nn.BatchNorm2d(num_features=n1)\n",
    "        self.do1 = torch.nn.Dropout(p=prob)\n",
    "        #self.pool1 = torch.nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=n1, out_channels=n2, kernel_size=2, padding=0, stride=(1,2))\n",
    "        self.ac2 = torch.nn.ReLU()\n",
    "        self.bn2 = torch.nn.BatchNorm2d(num_features=n2)\n",
    "        self.do2 = torch.nn.Dropout(p=prob)\n",
    "        \n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=n2, out_channels=n3, kernel_size=(2,3), padding=1, stride=(1,2))\n",
    "        self.ac3 = torch.nn.ReLU()\n",
    "        self.bn3 = torch.nn.BatchNorm2d(num_features=n3)\n",
    "        self.do3 = torch.nn.Dropout(p=prob)\n",
    "        \n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=n3, out_channels=n4, kernel_size=2, padding=0, stride=2)\n",
    "        self.ac4 = torch.nn.ReLU()\n",
    "        self.bn4 = torch.nn.BatchNorm2d(num_features=n4)\n",
    "        self.do4 = torch.nn.Dropout(p=prob)\n",
    "        self.pool4 = torch.nn.MaxPool2d(kernel_size=(2,1), stride=(2,1))\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(10*15*n4,200)\n",
    "        self.ac1_ = torch.nn.Tanh()\n",
    "        #self.bn1_ = torch.nn.BatchNorm1d(num_features=100)\n",
    "        self.fc2 = torch.nn.Linear(200,80)\n",
    "        self.ac2_ = torch.nn.Tanh()\n",
    "        #self.bn2_ = torch.nn.BatchNorm1d(num_features=40)\n",
    "        self.fc3 = torch.nn.Linear(80,20)\n",
    "        self.ac3_ = torch.nn.Tanh()\n",
    "        \n",
    "        self.out = torch.nn.Linear(20,3) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.ac1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.do1(x)\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "            \n",
    "        x = self.conv2(x)\n",
    "        x = self.ac2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.do2(x)\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "            \n",
    "        x = self.conv3(x)\n",
    "        x = self.ac3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.do3(x)\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "        #x = self.pool3(x)\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.ac4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.do4(x)\n",
    "        x = self.pool4(x)\n",
    "        #print(x.size(0),' x ', x.size(1),' x ', x.size(2),' x ', x.size(3))\n",
    "        \n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.ac1_(x)\n",
    "        #x = self.bn1_(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ac2_(x)\n",
    "        #x = self.bn2_(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.ac3_(x)\n",
    "            \n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "neural_net = FracAnisotropyNet(4,6,2)\n",
    "#neural_net = FracAnisotropyNet(6,8,6,4)\n",
    "neural_net = neural_net.to(device)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(neural_net.parameters(), lr=3.0e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 1 :\n",
      "\tTrain Loss:\t 0.12182518 \n",
      "\tTest Loss:\t 0.17902249\n",
      "epoch # 2 :\n",
      "\tTrain Loss:\t 0.018868022 \n",
      "\tTest Loss:\t 0.1001182\n",
      "epoch # 3 :\n",
      "\tTrain Loss:\t 0.0148056075 \n",
      "\tTest Loss:\t 0.030400885\n",
      "epoch # 4 :\n",
      "\tTrain Loss:\t 0.009233228 \n",
      "\tTest Loss:\t 0.0110942535\n",
      "epoch # 5 :\n",
      "\tTrain Loss:\t 0.0069666775 \n",
      "\tTest Loss:\t 0.008022499\n",
      "epoch # 6 :\n",
      "\tTrain Loss:\t 0.006316287 \n",
      "\tTest Loss:\t 0.006527923\n",
      "epoch # 7 :\n",
      "\tTrain Loss:\t 0.0056020967 \n",
      "\tTest Loss:\t 0.0059874393\n",
      "epoch # 8 :\n",
      "\tTrain Loss:\t 0.0051789726 \n",
      "\tTest Loss:\t 0.005752345\n",
      "epoch # 9 :\n",
      "\tTrain Loss:\t 0.004899408 \n",
      "\tTest Loss:\t 0.005527956\n",
      "epoch # 10 :\n",
      "\tTrain Loss:\t 0.004702624 \n",
      "\tTest Loss:\t 0.00525924\n",
      "epoch # 11 :\n",
      "\tTrain Loss:\t 0.004477335 \n",
      "\tTest Loss:\t 0.005111862\n",
      "epoch # 12 :\n",
      "\tTrain Loss:\t 0.00430419 \n",
      "\tTest Loss:\t 0.00491362\n",
      "epoch # 13 :\n",
      "\tTrain Loss:\t 0.004145968 \n",
      "\tTest Loss:\t 0.0047756378\n",
      "epoch # 14 :\n",
      "\tTrain Loss:\t 0.003971549 \n",
      "\tTest Loss:\t 0.004644756\n",
      "epoch # 15 :\n",
      "\tTrain Loss:\t 0.0038424274 \n",
      "\tTest Loss:\t 0.0045058345\n",
      "epoch # 16 :\n",
      "\tTrain Loss:\t 0.0037372673 \n",
      "\tTest Loss:\t 0.0043788548\n",
      "epoch # 17 :\n",
      "\tTrain Loss:\t 0.0036143835 \n",
      "\tTest Loss:\t 0.004232737\n",
      "epoch # 18 :\n",
      "\tTrain Loss:\t 0.0034446584 \n",
      "\tTest Loss:\t 0.0041193822\n",
      "epoch # 19 :\n",
      "\tTrain Loss:\t 0.003377893 \n",
      "\tTest Loss:\t 0.004024238\n",
      "epoch # 20 :\n",
      "\tTrain Loss:\t 0.0032270378 \n",
      "\tTest Loss:\t 0.0038969503\n",
      "epoch # 21 :\n",
      "\tTrain Loss:\t 0.0031511963 \n",
      "\tTest Loss:\t 0.003789819\n",
      "epoch # 22 :\n",
      "\tTrain Loss:\t 0.0030469017 \n",
      "\tTest Loss:\t 0.0037006957\n",
      "epoch # 23 :\n",
      "\tTrain Loss:\t 0.002941686 \n",
      "\tTest Loss:\t 0.003615114\n",
      "epoch # 24 :\n",
      "\tTrain Loss:\t 0.002854276 \n",
      "\tTest Loss:\t 0.0035052593\n",
      "epoch # 25 :\n",
      "\tTrain Loss:\t 0.0027388535 \n",
      "\tTest Loss:\t 0.0034243548\n",
      "epoch # 26 :\n",
      "\tTrain Loss:\t 0.002701253 \n",
      "\tTest Loss:\t 0.00332345\n",
      "epoch # 27 :\n",
      "\tTrain Loss:\t 0.0025964505 \n",
      "\tTest Loss:\t 0.0032446838\n",
      "epoch # 28 :\n",
      "\tTrain Loss:\t 0.0025165218 \n",
      "\tTest Loss:\t 0.003164381\n",
      "epoch # 29 :\n",
      "\tTrain Loss:\t 0.0024303126 \n",
      "\tTest Loss:\t 0.0030784886\n",
      "epoch # 30 :\n",
      "\tTrain Loss:\t 0.0023546605 \n",
      "\tTest Loss:\t 0.0029748\n",
      "epoch # 31 :\n",
      "\tTrain Loss:\t 0.0022954985 \n",
      "\tTest Loss:\t 0.0029395532\n",
      "epoch # 32 :\n",
      "\tTrain Loss:\t 0.0022106026 \n",
      "\tTest Loss:\t 0.0028182121\n",
      "epoch # 33 :\n",
      "\tTrain Loss:\t 0.0021212844 \n",
      "\tTest Loss:\t 0.0027510524\n",
      "epoch # 34 :\n",
      "\tTrain Loss:\t 0.002077946 \n",
      "\tTest Loss:\t 0.0026723382\n",
      "epoch # 35 :\n",
      "\tTrain Loss:\t 0.0019747962 \n",
      "\tTest Loss:\t 0.0026015993\n",
      "epoch # 36 :\n",
      "\tTrain Loss:\t 0.0019089619 \n",
      "\tTest Loss:\t 0.0025228453\n",
      "epoch # 37 :\n",
      "\tTrain Loss:\t 0.0018535185 \n",
      "\tTest Loss:\t 0.002453962\n",
      "epoch # 38 :\n",
      "\tTrain Loss:\t 0.0017760467 \n",
      "\tTest Loss:\t 0.0023789925\n",
      "epoch # 39 :\n",
      "\tTrain Loss:\t 0.001732865 \n",
      "\tTest Loss:\t 0.0023232952\n",
      "epoch # 40 :\n",
      "\tTrain Loss:\t 0.0016691438 \n",
      "\tTest Loss:\t 0.002258104\n",
      "epoch # 41 :\n",
      "\tTrain Loss:\t 0.0016322914 \n",
      "\tTest Loss:\t 0.0022130162\n",
      "epoch # 42 :\n",
      "\tTrain Loss:\t 0.0015812402 \n",
      "\tTest Loss:\t 0.002168823\n",
      "epoch # 43 :\n",
      "\tTrain Loss:\t 0.001496893 \n",
      "\tTest Loss:\t 0.002094539\n",
      "epoch # 44 :\n",
      "\tTrain Loss:\t 0.0014412834 \n",
      "\tTest Loss:\t 0.0020313684\n",
      "epoch # 45 :\n",
      "\tTrain Loss:\t 0.0013884916 \n",
      "\tTest Loss:\t 0.0019825087\n",
      "epoch # 46 :\n",
      "\tTrain Loss:\t 0.0013478056 \n",
      "\tTest Loss:\t 0.0019467402\n",
      "epoch # 47 :\n",
      "\tTrain Loss:\t 0.0013438587 \n",
      "\tTest Loss:\t 0.0018928938\n",
      "epoch # 48 :\n",
      "\tTrain Loss:\t 0.0012774714 \n",
      "\tTest Loss:\t 0.0018795703\n",
      "epoch # 49 :\n",
      "\tTrain Loss:\t 0.0012284806 \n",
      "\tTest Loss:\t 0.001820499\n",
      "epoch # 50 :\n",
      "\tTrain Loss:\t 0.001202964 \n",
      "\tTest Loss:\t 0.0018077857\n",
      "epoch # 51 :\n",
      "\tTrain Loss:\t 0.0011758409 \n",
      "\tTest Loss:\t 0.0017887282\n",
      "epoch # 52 :\n",
      "\tTrain Loss:\t 0.0011441268 \n",
      "\tTest Loss:\t 0.0017193791\n",
      "epoch # 53 :\n",
      "\tTrain Loss:\t 0.0010848748 \n",
      "\tTest Loss:\t 0.0016613873\n",
      "epoch # 54 :\n",
      "\tTrain Loss:\t 0.0010572191 \n",
      "\tTest Loss:\t 0.0016626548\n",
      "epoch # 55 :\n",
      "\tTrain Loss:\t 0.0010531741 \n",
      "\tTest Loss:\t 0.0016153418\n",
      "epoch # 56 :\n",
      "\tTrain Loss:\t 0.0010175704 \n",
      "\tTest Loss:\t 0.0016070277\n",
      "epoch # 57 :\n",
      "\tTrain Loss:\t 0.0009792832 \n",
      "\tTest Loss:\t 0.0015375435\n",
      "epoch # 58 :\n",
      "\tTrain Loss:\t 0.0009729239 \n",
      "\tTest Loss:\t 0.0015225579\n",
      "epoch # 59 :\n",
      "\tTrain Loss:\t 0.00092754635 \n",
      "\tTest Loss:\t 0.0014774898\n",
      "epoch # 60 :\n",
      "\tTrain Loss:\t 0.0008991704 \n",
      "\tTest Loss:\t 0.0014556793\n",
      "epoch # 61 :\n",
      "\tTrain Loss:\t 0.0008645264 \n",
      "\tTest Loss:\t 0.0014356409\n",
      "epoch # 62 :\n",
      "\tTrain Loss:\t 0.0008472934 \n",
      "\tTest Loss:\t 0.0014103685\n",
      "epoch # 63 :\n",
      "\tTrain Loss:\t 0.00083123485 \n",
      "\tTest Loss:\t 0.0013819545\n",
      "epoch # 64 :\n",
      "\tTrain Loss:\t 0.0008190422 \n",
      "\tTest Loss:\t 0.0013586564\n",
      "epoch # 65 :\n",
      "\tTrain Loss:\t 0.0008200809 \n",
      "\tTest Loss:\t 0.0014034276\n",
      "epoch # 66 :\n",
      "\tTrain Loss:\t 0.00079558924 \n",
      "\tTest Loss:\t 0.0013522052\n",
      "epoch # 67 :\n",
      "\tTrain Loss:\t 0.0007681405 \n",
      "\tTest Loss:\t 0.0013159227\n",
      "epoch # 68 :\n",
      "\tTrain Loss:\t 0.00073944184 \n",
      "\tTest Loss:\t 0.0012848057\n",
      "epoch # 69 :\n",
      "\tTrain Loss:\t 0.00071015256 \n",
      "\tTest Loss:\t 0.0012590461\n",
      "epoch # 70 :\n",
      "\tTrain Loss:\t 0.00069332053 \n",
      "\tTest Loss:\t 0.0012557706\n",
      "epoch # 71 :\n",
      "\tTrain Loss:\t 0.00069960457 \n",
      "\tTest Loss:\t 0.0012466118\n",
      "epoch # 72 :\n",
      "\tTrain Loss:\t 0.0006754121 \n",
      "\tTest Loss:\t 0.0012199559\n",
      "epoch # 73 :\n",
      "\tTrain Loss:\t 0.0006537064 \n",
      "\tTest Loss:\t 0.0012186907\n",
      "epoch # 74 :\n",
      "\tTrain Loss:\t 0.0006659311 \n",
      "\tTest Loss:\t 0.00125083\n",
      "epoch # 75 :\n",
      "\tTrain Loss:\t 0.00065562746 \n",
      "\tTest Loss:\t 0.0012615016\n",
      "epoch # 76 :\n",
      "\tTrain Loss:\t 0.00063913094 \n",
      "\tTest Loss:\t 0.0011812783\n",
      "epoch # 77 :\n",
      "\tTrain Loss:\t 0.0006243809 \n",
      "\tTest Loss:\t 0.0011450517\n",
      "epoch # 78 :\n",
      "\tTrain Loss:\t 0.000605005 \n",
      "\tTest Loss:\t 0.0011325013\n",
      "epoch # 79 :\n",
      "\tTrain Loss:\t 0.0005759952 \n",
      "\tTest Loss:\t 0.0011669835\n",
      "epoch # 80 :\n",
      "\tTrain Loss:\t 0.00058686576 \n",
      "\tTest Loss:\t 0.0011265797\n",
      "epoch # 81 :\n",
      "\tTrain Loss:\t 0.00057684095 \n",
      "\tTest Loss:\t 0.0010905359\n",
      "epoch # 82 :\n",
      "\tTrain Loss:\t 0.00054927304 \n",
      "\tTest Loss:\t 0.0010764275\n",
      "epoch # 83 :\n",
      "\tTrain Loss:\t 0.00055390754 \n",
      "\tTest Loss:\t 0.0010695336\n",
      "epoch # 84 :\n",
      "\tTrain Loss:\t 0.00053261983 \n",
      "\tTest Loss:\t 0.0010644136\n",
      "epoch # 85 :\n",
      "\tTrain Loss:\t 0.00051950046 \n",
      "\tTest Loss:\t 0.0010763003\n",
      "epoch # 86 :\n",
      "\tTrain Loss:\t 0.00051072246 \n",
      "\tTest Loss:\t 0.0010466364\n",
      "epoch # 87 :\n",
      "\tTrain Loss:\t 0.000512921 \n",
      "\tTest Loss:\t 0.0010299117\n",
      "epoch # 88 :\n",
      "\tTrain Loss:\t 0.0005263827 \n",
      "\tTest Loss:\t 0.0010620637\n",
      "epoch # 89 :\n",
      "\tTrain Loss:\t 0.00050152175 \n",
      "\tTest Loss:\t 0.0010996197\n",
      "epoch # 90 :\n",
      "\tTrain Loss:\t 0.00050554116 \n",
      "\tTest Loss:\t 0.0010392142\n",
      "epoch # 91 :\n",
      "\tTrain Loss:\t 0.0004914765 \n",
      "\tTest Loss:\t 0.0010515713\n",
      "epoch # 92 :\n",
      "\tTrain Loss:\t 0.0004657549 \n",
      "\tTest Loss:\t 0.0009903278\n",
      "epoch # 93 :\n",
      "\tTrain Loss:\t 0.00044510333 \n",
      "\tTest Loss:\t 0.000987346\n",
      "epoch # 94 :\n",
      "\tTrain Loss:\t 0.0004393081 \n",
      "\tTest Loss:\t 0.00097488565\n",
      "epoch # 95 :\n",
      "\tTrain Loss:\t 0.00043733258 \n",
      "\tTest Loss:\t 0.0009849394\n",
      "epoch # 96 :\n",
      "\tTrain Loss:\t 0.00045609157 \n",
      "\tTest Loss:\t 0.00096516014\n",
      "epoch # 97 :\n",
      "\tTrain Loss:\t 0.00042740413 \n",
      "\tTest Loss:\t 0.0009493095\n",
      "epoch # 98 :\n",
      "\tTrain Loss:\t 0.00041936283 \n",
      "\tTest Loss:\t 0.00094486045\n",
      "epoch # 99 :\n",
      "\tTrain Loss:\t 0.00041520444 \n",
      "\tTest Loss:\t 0.0009404459\n",
      "epoch # 100 :\n",
      "\tTrain Loss:\t 0.00042337456 \n",
      "\tTest Loss:\t 0.00093835103\n",
      "epoch # 101 :\n",
      "\tTrain Loss:\t 0.00043361582 \n",
      "\tTest Loss:\t 0.0009504783\n",
      "epoch # 102 :\n",
      "\tTrain Loss:\t 0.00042920109 \n",
      "\tTest Loss:\t 0.00094857276\n",
      "epoch # 103 :\n",
      "\tTrain Loss:\t 0.00042134826 \n",
      "\tTest Loss:\t 0.0009244745\n",
      "epoch # 104 :\n",
      "\tTrain Loss:\t 0.00040219745 \n",
      "\tTest Loss:\t 0.00095955364\n",
      "epoch # 105 :\n",
      "\tTrain Loss:\t 0.0003950601 \n",
      "\tTest Loss:\t 0.00092447887\n",
      "epoch # 106 :\n",
      "\tTrain Loss:\t 0.0003934383 \n",
      "\tTest Loss:\t 0.0009244951\n",
      "epoch # 107 :\n",
      "\tTrain Loss:\t 0.00039591527 \n",
      "\tTest Loss:\t 0.00095519784\n",
      "epoch # 108 :\n",
      "\tTrain Loss:\t 0.00040676884 \n",
      "\tTest Loss:\t 0.0008933647\n",
      "epoch # 109 :\n",
      "\tTrain Loss:\t 0.00036158474 \n",
      "\tTest Loss:\t 0.0008849669\n",
      "epoch # 110 :\n",
      "\tTrain Loss:\t 0.00036175895 \n",
      "\tTest Loss:\t 0.0008800979\n",
      "epoch # 111 :\n",
      "\tTrain Loss:\t 0.00035789798 \n",
      "\tTest Loss:\t 0.0008801203\n",
      "epoch # 112 :\n",
      "\tTrain Loss:\t 0.00035663592 \n",
      "\tTest Loss:\t 0.0008666838\n",
      "epoch # 113 :\n",
      "\tTrain Loss:\t 0.00034869695 \n",
      "\tTest Loss:\t 0.0008636298\n",
      "epoch # 114 :\n",
      "\tTrain Loss:\t 0.00036244205 \n",
      "\tTest Loss:\t 0.0008641557\n",
      "epoch # 115 :\n",
      "\tTrain Loss:\t 0.00036421086 \n",
      "\tTest Loss:\t 0.0008565338\n",
      "epoch # 116 :\n",
      "\tTrain Loss:\t 0.00034475763 \n",
      "\tTest Loss:\t 0.00084989914\n",
      "epoch # 117 :\n",
      "\tTrain Loss:\t 0.0003323801 \n",
      "\tTest Loss:\t 0.0008651808\n",
      "epoch # 118 :\n",
      "\tTrain Loss:\t 0.0003361604 \n",
      "\tTest Loss:\t 0.0008700061\n",
      "epoch # 119 :\n",
      "\tTrain Loss:\t 0.00032997102 \n",
      "\tTest Loss:\t 0.00084642944\n",
      "epoch # 120 :\n",
      "\tTrain Loss:\t 0.0003394987 \n",
      "\tTest Loss:\t 0.0008453524\n",
      "epoch # 121 :\n",
      "\tTrain Loss:\t 0.00032358553 \n",
      "\tTest Loss:\t 0.00084425934\n",
      "epoch # 122 :\n",
      "\tTrain Loss:\t 0.00032584753 \n",
      "\tTest Loss:\t 0.00083316653\n",
      "epoch # 123 :\n",
      "\tTrain Loss:\t 0.00033516082 \n",
      "\tTest Loss:\t 0.00090625597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 124 :\n",
      "\tTrain Loss:\t 0.00032073696 \n",
      "\tTest Loss:\t 0.00083744776\n",
      "epoch # 125 :\n",
      "\tTrain Loss:\t 0.00030983068 \n",
      "\tTest Loss:\t 0.0008194017\n",
      "epoch # 126 :\n",
      "\tTrain Loss:\t 0.00032338293 \n",
      "\tTest Loss:\t 0.0008403238\n",
      "epoch # 127 :\n",
      "\tTrain Loss:\t 0.00031959618 \n",
      "\tTest Loss:\t 0.0008169127\n",
      "epoch # 128 :\n",
      "\tTrain Loss:\t 0.00032732304 \n",
      "\tTest Loss:\t 0.00086654176\n",
      "epoch # 129 :\n",
      "\tTrain Loss:\t 0.00030315155 \n",
      "\tTest Loss:\t 0.0008303243\n",
      "epoch # 130 :\n",
      "\tTrain Loss:\t 0.00029605278 \n",
      "\tTest Loss:\t 0.0008232336\n",
      "epoch # 131 :\n",
      "\tTrain Loss:\t 0.00028874623 \n",
      "\tTest Loss:\t 0.00080213253\n",
      "epoch # 132 :\n",
      "\tTrain Loss:\t 0.00028850336 \n",
      "\tTest Loss:\t 0.0008356975\n",
      "epoch # 133 :\n",
      "\tTrain Loss:\t 0.00029531732 \n",
      "\tTest Loss:\t 0.0008274475\n",
      "epoch # 134 :\n",
      "\tTrain Loss:\t 0.0002894547 \n",
      "\tTest Loss:\t 0.0008049455\n",
      "epoch # 135 :\n",
      "\tTrain Loss:\t 0.00030719236 \n",
      "\tTest Loss:\t 0.0008464713\n",
      "epoch # 136 :\n",
      "\tTrain Loss:\t 0.00030563265 \n",
      "\tTest Loss:\t 0.0007827284\n",
      "epoch # 137 :\n",
      "\tTrain Loss:\t 0.00028750277 \n",
      "\tTest Loss:\t 0.00078928366\n",
      "epoch # 138 :\n",
      "\tTrain Loss:\t 0.0002848697 \n",
      "\tTest Loss:\t 0.0008259024\n",
      "epoch # 139 :\n",
      "\tTrain Loss:\t 0.00029259236 \n",
      "\tTest Loss:\t 0.0007930128\n",
      "epoch # 140 :\n",
      "\tTrain Loss:\t 0.00027757866 \n",
      "\tTest Loss:\t 0.00083085184\n",
      "epoch # 141 :\n",
      "\tTrain Loss:\t 0.00029663596 \n",
      "\tTest Loss:\t 0.00077685126\n",
      "epoch # 142 :\n",
      "\tTrain Loss:\t 0.0002786714 \n",
      "\tTest Loss:\t 0.00077913806\n",
      "epoch # 143 :\n",
      "\tTrain Loss:\t 0.00027747694 \n",
      "\tTest Loss:\t 0.00080709584\n",
      "epoch # 144 :\n",
      "\tTrain Loss:\t 0.00027400831 \n",
      "\tTest Loss:\t 0.000804235\n",
      "epoch # 145 :\n",
      "\tTrain Loss:\t 0.00026983384 \n",
      "\tTest Loss:\t 0.00075963367\n",
      "epoch # 146 :\n",
      "\tTrain Loss:\t 0.00027443288 \n",
      "\tTest Loss:\t 0.00078429974\n",
      "epoch # 147 :\n",
      "\tTrain Loss:\t 0.00029329426 \n",
      "\tTest Loss:\t 0.0007983104\n",
      "epoch # 148 :\n",
      "\tTrain Loss:\t 0.0002639555 \n",
      "\tTest Loss:\t 0.0007713976\n",
      "epoch # 149 :\n",
      "\tTrain Loss:\t 0.0002621357 \n",
      "\tTest Loss:\t 0.00076032884\n",
      "epoch # 150 :\n",
      "\tTrain Loss:\t 0.00024988037 \n",
      "\tTest Loss:\t 0.0007515964\n",
      "epoch # 151 :\n",
      "\tTrain Loss:\t 0.000244018 \n",
      "\tTest Loss:\t 0.0007468962\n",
      "epoch # 152 :\n",
      "\tTrain Loss:\t 0.0002500829 \n",
      "\tTest Loss:\t 0.000756166\n",
      "epoch # 153 :\n",
      "\tTrain Loss:\t 0.0002443337 \n",
      "\tTest Loss:\t 0.0007702592\n",
      "epoch # 154 :\n",
      "\tTrain Loss:\t 0.0002516405 \n",
      "\tTest Loss:\t 0.0007575017\n",
      "epoch # 155 :\n",
      "\tTrain Loss:\t 0.00025554994 \n",
      "\tTest Loss:\t 0.00077305274\n",
      "epoch # 156 :\n",
      "\tTrain Loss:\t 0.00026293407 \n",
      "\tTest Loss:\t 0.0007575978\n",
      "epoch # 157 :\n",
      "\tTrain Loss:\t 0.00024699236 \n",
      "\tTest Loss:\t 0.0007486301\n",
      "epoch # 158 :\n",
      "\tTrain Loss:\t 0.0002489249 \n",
      "\tTest Loss:\t 0.00074065314\n",
      "epoch # 159 :\n",
      "\tTrain Loss:\t 0.00023439914 \n",
      "\tTest Loss:\t 0.0007330069\n",
      "epoch # 160 :\n",
      "\tTrain Loss:\t 0.00022778117 \n",
      "\tTest Loss:\t 0.00073635846\n",
      "epoch # 161 :\n",
      "\tTrain Loss:\t 0.00022609186 \n",
      "\tTest Loss:\t 0.0007409642\n",
      "epoch # 162 :\n",
      "\tTrain Loss:\t 0.00022344416 \n",
      "\tTest Loss:\t 0.00072466815\n",
      "epoch # 163 :\n",
      "\tTrain Loss:\t 0.00022443786 \n",
      "\tTest Loss:\t 0.0007221506\n",
      "epoch # 164 :\n",
      "\tTrain Loss:\t 0.0002292542 \n",
      "\tTest Loss:\t 0.0007240716\n",
      "epoch # 165 :\n",
      "\tTrain Loss:\t 0.00022241891 \n",
      "\tTest Loss:\t 0.00071841135\n",
      "epoch # 166 :\n",
      "\tTrain Loss:\t 0.00021559524 \n",
      "\tTest Loss:\t 0.0007226611\n",
      "epoch # 167 :\n",
      "\tTrain Loss:\t 0.00021988992 \n",
      "\tTest Loss:\t 0.00073197857\n",
      "epoch # 168 :\n",
      "\tTrain Loss:\t 0.00022892938 \n",
      "\tTest Loss:\t 0.00072119344\n",
      "epoch # 169 :\n",
      "\tTrain Loss:\t 0.00021151669 \n",
      "\tTest Loss:\t 0.0007100526\n",
      "epoch # 170 :\n",
      "\tTrain Loss:\t 0.0002150657 \n",
      "\tTest Loss:\t 0.00072369986\n",
      "epoch # 171 :\n",
      "\tTrain Loss:\t 0.00023320763 \n",
      "\tTest Loss:\t 0.00072082155\n",
      "epoch # 172 :\n",
      "\tTrain Loss:\t 0.00021131609 \n",
      "\tTest Loss:\t 0.0007316168\n",
      "epoch # 173 :\n",
      "\tTrain Loss:\t 0.00021746226 \n",
      "\tTest Loss:\t 0.000699021\n",
      "epoch # 174 :\n",
      "\tTrain Loss:\t 0.00021881264 \n",
      "\tTest Loss:\t 0.0007030879\n",
      "epoch # 175 :\n",
      "\tTrain Loss:\t 0.00020483261 \n",
      "\tTest Loss:\t 0.0006965969\n",
      "epoch # 176 :\n",
      "\tTrain Loss:\t 0.00021770035 \n",
      "\tTest Loss:\t 0.0006957104\n",
      "epoch # 177 :\n",
      "\tTrain Loss:\t 0.000201878 \n",
      "\tTest Loss:\t 0.00070401165\n",
      "epoch # 178 :\n",
      "\tTrain Loss:\t 0.00022085878 \n",
      "\tTest Loss:\t 0.0007440241\n",
      "epoch # 179 :\n",
      "\tTrain Loss:\t 0.00022092956 \n",
      "\tTest Loss:\t 0.000702912\n",
      "epoch # 180 :\n",
      "\tTrain Loss:\t 0.0002088511 \n",
      "\tTest Loss:\t 0.0007104327\n",
      "epoch # 181 :\n",
      "\tTrain Loss:\t 0.00021544361 \n",
      "\tTest Loss:\t 0.00072271563\n",
      "epoch # 182 :\n",
      "\tTrain Loss:\t 0.00020586654 \n",
      "\tTest Loss:\t 0.0007017482\n",
      "epoch # 183 :\n",
      "\tTrain Loss:\t 0.00020041305 \n",
      "\tTest Loss:\t 0.00069336017\n",
      "epoch # 184 :\n",
      "\tTrain Loss:\t 0.00019499424 \n",
      "\tTest Loss:\t 0.000692096\n",
      "epoch # 185 :\n",
      "\tTrain Loss:\t 0.00020885869 \n",
      "\tTest Loss:\t 0.00070055824\n",
      "epoch # 186 :\n",
      "\tTrain Loss:\t 0.00021635444 \n",
      "\tTest Loss:\t 0.0006899007\n",
      "epoch # 187 :\n",
      "\tTrain Loss:\t 0.00019895427 \n",
      "\tTest Loss:\t 0.0006788716\n",
      "epoch # 188 :\n",
      "\tTrain Loss:\t 0.00019195542 \n",
      "\tTest Loss:\t 0.0007059353\n",
      "epoch # 189 :\n",
      "\tTrain Loss:\t 0.00021822393 \n",
      "\tTest Loss:\t 0.0007136018\n",
      "epoch # 190 :\n",
      "\tTrain Loss:\t 0.0002077627 \n",
      "\tTest Loss:\t 0.000689673\n",
      "epoch # 191 :\n",
      "\tTrain Loss:\t 0.00019312113 \n",
      "\tTest Loss:\t 0.0006979254\n",
      "epoch # 192 :\n",
      "\tTrain Loss:\t 0.0002036231 \n",
      "\tTest Loss:\t 0.00074041134\n",
      "epoch # 193 :\n",
      "\tTrain Loss:\t 0.00020367558 \n",
      "\tTest Loss:\t 0.00072334165\n",
      "epoch # 194 :\n",
      "\tTrain Loss:\t 0.0002003972 \n",
      "\tTest Loss:\t 0.0006772645\n",
      "epoch # 195 :\n",
      "\tTrain Loss:\t 0.00019568918 \n",
      "\tTest Loss:\t 0.00069357915\n",
      "epoch # 196 :\n",
      "\tTrain Loss:\t 0.00019301582 \n",
      "\tTest Loss:\t 0.0006951764\n",
      "epoch # 197 :\n",
      "\tTrain Loss:\t 0.00019912797 \n",
      "\tTest Loss:\t 0.0006761538\n",
      "epoch # 198 :\n",
      "\tTrain Loss:\t 0.00019259461 \n",
      "\tTest Loss:\t 0.00069914234\n",
      "epoch # 199 :\n",
      "\tTrain Loss:\t 0.00019021131 \n",
      "\tTest Loss:\t 0.0006766676\n",
      "epoch # 200 :\n",
      "\tTrain Loss:\t 0.00018162835 \n",
      "\tTest Loss:\t 0.00066923123\n",
      "Time elapsed: 0:08:46.882946\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "N = len(X_train)\n",
    "\n",
    "test_loss_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "X_validation = X_validation.to(device)\n",
    "Y_validation = Y_validation.to(device)\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    order = np.random.permutation(N)\n",
    "  \n",
    "    loss_sum = 0.0\n",
    "    for batch_init in range(0,N,batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        neural_net.train()\n",
    "    \n",
    "        X_batch = X_train[order[batch_init:batch_init+batch_size]].to(device)\n",
    "        y_batch = Y_train[order[batch_init:batch_init+batch_size]].to(device)\n",
    "    \n",
    "        pred = neural_net.forward(X_batch)\n",
    "        #loss_value = weighted_loss(pred,y_batch)\n",
    "        loss_value = loss(pred,y_batch)\n",
    "        loss_sum += loss_value\n",
    "        loss_value.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "    \n",
    "    neural_net.eval()\n",
    "    pred = neural_net.forward(X_validation)\n",
    "  \n",
    "    tr_l = loss_sum/(N/batch_size)\n",
    "    te_l = loss(pred,Y_validation)\n",
    "    train_loss_history.append(tr_l.data.cpu())\n",
    "    test_loss_history.append(te_l.data.cpu())\n",
    "  \n",
    "    print('epoch #', epoch+1,':\\n\\tTrain Loss:\\t',tr_l.data.cpu().numpy(),'\\n\\tTest Loss:\\t',te_l.data.cpu().numpy())\n",
    "\n",
    "educ_time = datetime.datetime.now() - start_time\n",
    "print('Time elapsed:', educ_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "num = 30\n",
    "def moving_average(a, n=num): # среднее скользящее среднее\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2416e031988>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_history[10:])\n",
    "plt.plot(test_loss_history[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2410bc25088>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_loss_train = moving_average(train_loss_history)\n",
    "avg_loss_test = moving_average(test_loss_history)\n",
    "plt.plot(avg_loss_train[10:])\n",
    "plt.plot(avg_loss_test[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted\n",
      "Real value\n",
      "Difference\n",
      "\n",
      "[0.04551761 0.09351505 0.00704949] \n",
      " [0.05739814 0.18541272 0.00908299] \n",
      " [0.01188053 0.09189767 0.0020335 ] \n",
      "\n",
      "[0.12187547 0.02630506 0.01655552] \n",
      " [0.14326808 0.0504863  0.0226715 ] \n",
      " [0.02139261 0.02418124 0.00611598] \n",
      "\n",
      "[0.38201112 0.12999809 0.05948746] \n",
      " [0.38801473 0.13673267 0.06140151] \n",
      " [0.00600362 0.00673458 0.00191405] \n",
      "\n",
      "[0.50312614 0.17815039 0.079395  ] \n",
      " [0.4949551  0.17441742 0.07832433] \n",
      " [0.00817105 0.00373296 0.00107068] \n",
      "\n",
      "[0.14337984 0.04231836 0.02069259] \n",
      " [0.13789359 0.04859238 0.02182101] \n",
      " [0.00548625 0.00627401 0.00112842] \n",
      "\n",
      "[0.11981811 0.04406575 0.01863204] \n",
      " [0.11267845 0.0397068  0.01783084] \n",
      " [0.00713966 0.00435895 0.0008012 ] \n",
      "\n",
      "[0.05259866 0.08412983 0.00827425] \n",
      " [0.05206763 0.11454269 0.00823946] \n",
      " [5.3102523e-04 3.0412860e-02 3.4787692e-05] \n",
      "\n",
      "[0.24441569 0.08452804 0.03870798] \n",
      " [0.24150485 0.08510399 0.03821701] \n",
      " [0.00291084 0.00057594 0.00049097] \n",
      "\n",
      "[0.04733258 0.0816751  0.00380926] \n",
      " [0.01217017 0.02443651 0.00192587] \n",
      " [0.03516241 0.05723859 0.00188338] \n",
      "\n",
      "[0.11054165 0.05397479 0.01468337] \n",
      " [0.10579438 0.03728092 0.01674147] \n",
      " [0.00474727 0.01669386 0.0020581 ] \n",
      "\n",
      "[0.35617453 0.11985128 0.05667748] \n",
      " [0.34922707 0.12306426 0.05526355] \n",
      " [0.00694746 0.00321299 0.00141393] \n",
      "\n",
      "[0.23375395 0.08628866 0.03750268] \n",
      " [0.23211284 0.08179434 0.03673077] \n",
      " [0.00164111 0.00449432 0.00077191] \n",
      "\n",
      "[0.1365631 0.0427399 0.0211194] \n",
      " [0.12605287 0.04441982 0.01994728] \n",
      " [0.01051024 0.00167992 0.00117212] \n",
      "\n",
      "[0.34382033 0.12303474 0.05478771] \n",
      " [0.34146178 0.12032785 0.05403472] \n",
      " [0.00235856 0.00270689 0.00075299] \n",
      "\n",
      "[0.5473012  0.1927106  0.08526845] \n",
      " [0.52933854 0.18653381 0.08376534] \n",
      " [0.01796263 0.00617678 0.00150311] \n",
      "\n",
      "[0.14525914 0.04291122 0.02983153] \n",
      " [0.21510792 0.07580197 0.03403982] \n",
      " [0.06984878 0.03289074 0.00420829] \n",
      "\n",
      "[0.3887694  0.13960138 0.06039856] \n",
      " [0.38280615 0.1348972  0.06057728] \n",
      " [0.00596324 0.00470418 0.00017872] \n",
      "\n",
      "[0.4866523  0.17259438 0.07685269] \n",
      " [0.4853889  0.17104639 0.07681052] \n",
      " [1.2634099e-03 1.5479922e-03 4.2177737e-05] \n",
      "\n",
      "[0.1787465  0.06510336 0.02750144] \n",
      " [0.17844193 0.06288122 0.0282376 ] \n",
      " [0.00030458 0.00222214 0.00073616] \n",
      "\n",
      "[0.23692799 0.08201458 0.03736283] \n",
      " [0.23807083 0.08389387 0.03767359] \n",
      " [0.00114284 0.0018793  0.00031076] \n",
      "\n",
      "[0.03252751 0.11541252 0.00729039] \n",
      " [0.04915874 0.0057335  0.00777914] \n",
      " [0.01663123 0.10967901 0.00048875] \n",
      "\n",
      "[0.33228046 0.11844935 0.05238535] \n",
      " [0.3334429  0.11750208 0.05276578] \n",
      " [0.00116244 0.00094727 0.00038043] \n",
      "\n",
      "[0.14265075 0.04964885 0.02279439] \n",
      " [0.1488284 0.0524457 0.0235514] \n",
      " [0.00617765 0.00279685 0.00075701] \n",
      "\n",
      "[0.5064751  0.17938209 0.07899629] \n",
      " [0.49593186 0.17476162 0.07847889] \n",
      " [0.01054323 0.00462046 0.0005174 ] \n",
      "\n",
      "[0.17063034 0.060295   0.02628557] \n",
      " [0.17635305 0.06214512 0.02790704] \n",
      " [0.00572272 0.00185012 0.00162148] \n",
      "\n",
      "[0.32949138 0.11994898 0.05352702] \n",
      " [0.33037806 0.11642206 0.05228078] \n",
      " [0.00088668 0.00352693 0.00124624] \n",
      "\n",
      "[0.38744992 0.13573031 0.05971757] \n",
      " [0.3866099  0.13623762 0.06117921] \n",
      " [0.00084001 0.00050731 0.00146163] \n",
      "\n",
      "[0.37702918 0.1275903  0.05782522] \n",
      " [0.372097   0.13112341 0.05888261] \n",
      " [0.00493219 0.00353311 0.00105739] \n",
      "\n",
      "[0.02723549 0.16410013 0.00410167] \n",
      " [0.02650275 0.16173592 0.00419394] \n",
      " [7.3273852e-04 2.3642033e-03 9.2269387e-05] \n",
      "\n",
      "[0.41323233 0.14813232 0.06575249] \n",
      " [0.4136233  0.14575689 0.06545395] \n",
      " [0.00039098 0.00237544 0.00029854] \n",
      "\n",
      "[0.45347726 0.16032617 0.07151741] \n",
      " [0.4476335  0.15774176 0.0708359 ] \n",
      " [0.00584376 0.00258441 0.0006815 ] \n",
      "\n",
      "[0.4772495  0.16955523 0.07563736] \n",
      " [0.46585557 0.16416302 0.07371946] \n",
      " [0.01139393 0.00539221 0.0019179 ] \n",
      "\n",
      "[0.60307795 0.2129148  0.09529167] \n",
      " [0.5948185  0.20960832 0.09412723] \n",
      " [0.00825948 0.00330648 0.00116444] \n",
      "\n",
      "[0.57153046 0.202979   0.08969986] \n",
      " [0.5599704  0.19732818 0.08861269] \n",
      " [0.01156008 0.00565082 0.00108717] \n",
      "\n",
      "[0.44757223 0.15700564 0.07056897] \n",
      " [0.442846   0.15605468 0.0700783 ] \n",
      " [0.00472623 0.00095096 0.00049067] \n",
      "\n",
      "[0.45113957 0.16075936 0.07189953] \n",
      " [0.44866297 0.15810451 0.07099881] \n",
      " [0.0024766  0.00265485 0.00090072] \n",
      "\n",
      "[0.5777685  0.20372689 0.09071679] \n",
      " [0.5747635  0.20254114 0.09095363] \n",
      " [0.00300503 0.00118575 0.00023684] \n",
      "\n",
      "[0.05940642 0.07045834 0.01070681] \n",
      " [0.06427149 0.07636817 0.01017066] \n",
      " [0.00486507 0.00590983 0.00053615] \n",
      "\n",
      "[0.5518257  0.19509482 0.08668581] \n",
      " [0.54507655 0.19207975 0.08625581] \n",
      " [0.00674915 0.00301507 0.00043   ] \n",
      "\n",
      "[0.4956841  0.17580104 0.07768171] \n",
      " [0.49073216 0.1729293  0.07765606] \n",
      " [4.9519241e-03 2.8717369e-03 2.5652349e-05] \n",
      "\n",
      "[0.5870392  0.20706022 0.09254758] \n",
      " [0.58740264 0.20699506 0.09295372] \n",
      " [3.6346912e-04 6.5162778e-05 4.0613860e-04] \n",
      "\n",
      "[0.43674266 0.1516846  0.06847103] \n",
      " [0.42864132 0.15104908 0.06783048] \n",
      " [0.00810134 0.00063552 0.00064055] \n",
      "\n",
      "[0.14395118 0.04403588 0.02076815] \n",
      " [0.1420386  0.05005304 0.02247694] \n",
      " [0.00191258 0.00601716 0.00170879] \n",
      "\n",
      "[0.29819727 0.09361292 0.04474396] \n",
      " [0.29167297 0.10278275 0.04615588] \n",
      " [0.00652429 0.00916982 0.00141192] \n",
      "\n",
      "[0.02096951 0.17482011 0.00214732] \n",
      " [0.01711706 0.18496355 0.00270869] \n",
      " [0.00385245 0.01014344 0.00056138] \n",
      "\n",
      "[0.22024475 0.07328724 0.03346198] \n",
      " [0.21733181 0.07658564 0.03439174] \n",
      " [0.00291294 0.00329839 0.00092976] \n",
      "\n",
      "[0.18033685 0.06406632 0.02541444] \n",
      " [0.18245284 0.06429463 0.02887231] \n",
      " [0.00211599 0.00022831 0.00345787] \n",
      "\n",
      "[0.609761   0.21609652 0.09688234] \n",
      " [0.62478197 0.22016719 0.09886882] \n",
      " [0.01502097 0.00407067 0.00198649] \n",
      "\n",
      "[0.02059926 0.1180706  0.00336094] \n",
      " [0.018155   0.12089197 0.00287294] \n",
      " [0.00244427 0.00282137 0.000488  ] \n",
      "\n",
      "[0.44040883 0.15516193 0.06933118] \n",
      " [0.4376919  0.15423842 0.06926269] \n",
      " [2.7169287e-03 9.2351437e-04 6.8485737e-05] \n",
      "\n",
      "[0.4196661  0.14563647 0.06634603] \n",
      " [0.4174695  0.14711224 0.06606259] \n",
      " [0.00219661 0.00147577 0.00028344] \n",
      "\n",
      "[0.41428858 0.14629431 0.06576851] \n",
      " [0.41172695 0.14508863 0.06515386] \n",
      " [0.00256163 0.00120568 0.00061465] \n",
      "\n",
      "[0.4892233  0.17012028 0.07617074] \n",
      " [0.48084396 0.16944478 0.0760913 ] \n",
      " [8.3793402e-03 6.7549944e-04 7.9438090e-05] \n",
      "\n",
      "[0.04012452 0.1982884  0.00514358] \n",
      " [0.03098537 0.19630837 0.00490329] \n",
      " [0.00913915 0.00198002 0.00024029] \n",
      "\n",
      "[0.5090626  0.17812677 0.07942928] \n",
      " [0.4964228  0.17493463 0.07855658] \n",
      " [0.01263979 0.00319214 0.0008727 ] \n",
      "\n",
      "[0.5966967  0.20969748 0.0936724 ] \n",
      " [0.5849729  0.20613885 0.09256922] \n",
      " [0.01172376 0.00355864 0.00110319] \n",
      "\n",
      "[0.3320846  0.12215187 0.05183689] \n",
      " [0.32403818 0.11418794 0.05127752] \n",
      " [0.00804642 0.00796393 0.00055937] \n",
      "\n",
      "[0.57753855 0.20347945 0.09035871] \n",
      " [0.5679486  0.20013961 0.08987519] \n",
      " [0.00958997 0.00333984 0.00048352] \n",
      "\n",
      "[0.3649854  0.12586038 0.05733998] \n",
      " [0.36021662 0.12693688 0.05700259] \n",
      " [0.00476879 0.0010765  0.00033739] \n",
      "\n",
      "[0.50019157 0.1769185  0.07934936] \n",
      " [0.4946316  0.17430343 0.07827313] \n",
      " [0.00555998 0.00261508 0.00107623] \n",
      "\n",
      "[0.29588813 0.10221913 0.04519702] \n",
      " [0.2874995  0.10131206 0.04549544] \n",
      " [0.00838864 0.00090707 0.00029842] \n",
      "\n",
      "[0.6116804  0.21642163 0.09772594] \n",
      " [0.62462634 0.22011232 0.09884419] \n",
      " [0.01294595 0.00369069 0.00111825] \n",
      "\n",
      "[0.37131202 0.13247475 0.05938957] \n",
      " [0.37692022 0.13282308 0.05964586] \n",
      " [0.0056082  0.00034833 0.00025629] \n",
      "\n",
      "[0.3820029  0.13718835 0.05890626] \n",
      " [0.37711522 0.13289179 0.05967671] \n",
      " [0.00488767 0.00429656 0.00077046] \n",
      "\n",
      "[0.05090828 0.10933177 0.00784661] \n",
      " [0.06734404 0.11458927 0.01065688] \n",
      " [0.01643576 0.0052575  0.00281027] \n",
      "\n",
      "[0.3506683  0.12353165 0.05399188] \n",
      " [0.3486442  0.12285886 0.05517131] \n",
      " [0.00202411 0.00067279 0.00117942] \n",
      "\n",
      "[0.17539948 0.07802638 0.02521051] \n",
      " [0.18982936 0.06689404 0.0300396 ] \n",
      " [0.01442988 0.01113234 0.0048291 ] \n",
      "\n",
      "[0.08877824 0.09862816 0.0133947 ] \n",
      " [0.0879254  0.10523462 0.01391378] \n",
      " [0.00085283 0.00660647 0.00051908] \n",
      "\n",
      "[0.4284346  0.1496289  0.06714354] \n",
      " [0.4277175  0.15072355 0.06768429] \n",
      " [0.0007171  0.00109464 0.00054076] \n",
      "\n",
      "[0.32054687 0.11627734 0.05038356] \n",
      " [0.31902587 0.11242165 0.05048435] \n",
      " [0.00152099 0.00385568 0.00010079] \n",
      "\n",
      "[0.24317575 0.07997568 0.03721813] \n",
      " [0.23290898 0.08207489 0.03685676] \n",
      " [0.01026677 0.00209921 0.00036137] \n",
      "\n",
      "[0.37038922 0.13122149 0.0574195 ] \n",
      " [0.37126184 0.1308291  0.05875044] \n",
      " [0.00087261 0.00039239 0.00133094] \n",
      "\n",
      "[0.20524566 0.07134868 0.03182391] \n",
      " [0.2005543  0.07067341 0.03173678] \n",
      " [4.6913624e-03 6.7526847e-04 8.7138265e-05] \n",
      "\n",
      "[0.45323902 0.16097674 0.07210045] \n",
      " [0.4498423  0.1585201  0.07118543] \n",
      " [0.00339672 0.00245664 0.00091502] \n",
      "\n",
      "[0.12134162 0.04096789 0.01984307] \n",
      " [0.12551616 0.04423069 0.01986234] \n",
      " [4.1745380e-03 3.2628030e-03 1.9274652e-05] \n",
      "\n",
      "[0.27546674 0.0968067  0.03508115] \n",
      " [0.28214622 0.09942562 0.04464832] \n",
      " [0.00667948 0.00261892 0.00956716] \n",
      "\n",
      "[0.46654862 0.16237374 0.07280683] \n",
      " [0.45892987 0.16172247 0.0726235 ] \n",
      " [0.00761876 0.00065127 0.00018333] \n",
      "\n",
      "[0.11895026 0.04207086 0.01714271] \n",
      " [0.11299563 0.03981858 0.01788103] \n",
      " [0.00595463 0.00225228 0.00073832] \n",
      "\n",
      "[0.25714523 0.08638202 0.04012948] \n",
      " [0.25210762 0.08884031 0.03989485] \n",
      " [0.00503761 0.00245828 0.00023463] \n",
      "\n",
      "[0.15934147 0.04724053 0.02335054] \n",
      " [0.14785223 0.05210171 0.02339692] \n",
      " [1.1489242e-02 4.8611835e-03 4.6385452e-05] \n",
      "\n",
      "[0.10903926 0.03710569 0.01729986] \n",
      " [0.11166185 0.03934857 0.01766996] \n",
      " [0.00262259 0.00224288 0.0003701 ] \n",
      "\n",
      "[0.0735347  0.07755341 0.01128453] \n",
      " [0.05523267 0.07872643 0.00874031] \n",
      " [0.01830202 0.00117302 0.00254421] \n",
      "\n",
      "[0.11783533 0.03229225 0.01724003] \n",
      " [0.11818814 0.04164837 0.01870272] \n",
      " [0.00035281 0.00935611 0.00146269] \n",
      "\n",
      "[0.36759508 0.125128   0.056852  ] \n",
      " [0.36790758 0.1296471  0.05821965] \n",
      " [0.00031251 0.0045191  0.00136765] \n",
      "\n",
      "[0.47508383 0.1654242  0.07435498] \n",
      " [0.47021085 0.16569778 0.07440867] \n",
      " [4.8729777e-03 2.7358532e-04 5.3681433e-05] \n",
      "\n",
      "[0.10656191 0.03270518 0.01337061] \n",
      " [0.10400342 0.03664981 0.01645805] \n",
      " [0.00255848 0.00394463 0.00308744] \n",
      "\n",
      "[0.56836724 0.20144913 0.08891503] \n",
      " [0.55841273 0.1967793  0.0883662 ] \n",
      " [0.00995451 0.00466983 0.00054882] \n",
      "\n",
      "[0.3794278  0.13398322 0.06026103] \n",
      " [0.37245375 0.13124911 0.05893906] \n",
      " [0.00697404 0.00273411 0.00132198] \n",
      "\n",
      "[0.16726686 0.04529515 0.02497469] \n",
      " [0.16605155 0.05851497 0.02627688] \n",
      " [0.00121531 0.01321982 0.00130219] \n",
      "\n",
      "[0.0205408 0.1903274 0.0047827] \n",
      " [0.00584254 0.22029725 0.00092455] \n",
      " [0.01469826 0.02996984 0.00385815] \n",
      "\n",
      "[0.48018783 0.1692411  0.07514177] \n",
      " [0.47798094 0.1684359  0.07563824] \n",
      " [0.00220689 0.0008052  0.00049648] \n",
      "\n",
      "[0.1741363  0.06536831 0.03054755] \n",
      " [0.17208098 0.06063969 0.02723101] \n",
      " [0.00205532 0.00472862 0.00331654] \n",
      "\n",
      "[0.09841252 0.03191809 0.01619541] \n",
      " [0.10434419 0.03676989 0.01651198] \n",
      " [0.00593167 0.0048518  0.00031656] \n",
      "\n",
      "[0.56677675 0.19933586 0.08938509] \n",
      " [0.55623716 0.19601263 0.08802193] \n",
      " [0.01053959 0.00332323 0.00136317] \n",
      "\n",
      "[0.51249063 0.1818194  0.08146623] \n",
      " [0.47805536 0.16846213 0.07565002] \n",
      " [0.03443527 0.01335727 0.00581621] \n",
      "\n",
      "[0.32537615 0.11636536 0.05048274] \n",
      " [0.32600045 0.11487943 0.05158804] \n",
      " [0.0006243  0.00148593 0.0011053 ] \n",
      "\n",
      "[0.2948603  0.09972198 0.04774868] \n",
      " [0.2951462  0.10400669 0.0467055 ] \n",
      " [0.00028589 0.0042847  0.00104318] \n",
      "\n",
      "[0.06039035 0.12386277 0.00986285] \n",
      " [0.05781041 0.13830139 0.00914823] \n",
      " [0.00257994 0.01443862 0.00071463] \n",
      "\n",
      "[0.21176766 0.07154428 0.03352679] \n",
      " [0.20687775 0.07290173 0.03273743] \n",
      " [0.00488991 0.00135745 0.00078936] \n",
      "\n",
      "[0.02883145 0.18625712 0.00307325] \n",
      " [0.02444473 0.17985967 0.00386826] \n",
      " [0.00438673 0.00639746 0.00079502] \n",
      "\n",
      "[0.3127631  0.09939501 0.04892973] \n",
      " [0.3120518  0.10996406 0.04938073] \n",
      " [0.00071129 0.01056905 0.00045101] \n",
      "\n",
      "[0.16019706 0.05636977 0.02472518] \n",
      " [0.15943769 0.05618431 0.02523027] \n",
      " [0.00075938 0.00018546 0.00050508] \n",
      "\n",
      "[0.0302856  0.06008254 0.00462313] \n",
      " [0.01749596 0.0487416  0.00276865] \n",
      " [0.01278965 0.01134093 0.00185448] \n",
      "\n",
      "[0.20853947 0.06656978 0.03577563] \n",
      " [0.1920459  0.06767513 0.03039036] \n",
      " [0.01649357 0.00110535 0.00538527] \n",
      "\n",
      "[0.1665186  0.05638234 0.02852074] \n",
      " [0.14988126 0.05281671 0.02371801] \n",
      " [0.01663734 0.00356563 0.00480273] \n",
      "\n",
      "[0.2334676  0.08369522 0.03618   ] \n",
      " [0.22265713 0.07846224 0.03523445] \n",
      " [0.01081046 0.00523298 0.00094556] \n",
      "\n",
      "[0.28729895 0.10100577 0.04495286] \n",
      " [0.28305665 0.09974644 0.04479238] \n",
      " [0.0042423  0.00125933 0.00016048] \n",
      "\n",
      "[0.3610595  0.12949185 0.0560316 ] \n",
      " [0.35130587 0.12379681 0.0555925 ] \n",
      " [0.00975361 0.00569504 0.00043909] \n",
      "\n",
      "[0.27342778 0.09971413 0.04211064] \n",
      " [0.26869357 0.09468503 0.0425195 ] \n",
      " [0.00473422 0.0050291  0.00040885] \n",
      "\n",
      "[0.11996128 0.04431466 0.01867371] \n",
      " [0.11254154 0.03965856 0.01780917] \n",
      " [0.00741974 0.00465611 0.00086454] \n",
      "\n",
      "[0.30467898 0.10879872 0.04885183] \n",
      " [0.31603366 0.11136723 0.05001085] \n",
      " [0.01135468 0.00256851 0.00115902] \n",
      "\n",
      "[0.07631674 0.16253456 0.0126033 ] \n",
      " [0.0949632  0.16142276 0.01502748] \n",
      " [0.01864646 0.00111181 0.00242418] \n",
      "\n",
      "[0.46283114 0.15765333 0.07165981] \n",
      " [0.45556203 0.16053568 0.07209055] \n",
      " [0.00726911 0.00288235 0.00043074] \n",
      "\n",
      "[0.04317132 0.08293112 0.00411872] \n",
      " [0.01172577 0.01268472 0.00185555] \n",
      " [0.03144555 0.07024639 0.00226318] \n",
      "\n",
      "[0.10428772 0.04029635 0.01661538] \n",
      " [0.11116681 0.03917412 0.01759163] \n",
      " [0.00687909 0.00112223 0.00097625] \n",
      "\n",
      "[0.44414967 0.15456411 0.06911685] \n",
      " [0.4386811  0.154587   0.06941923] \n",
      " [5.4685771e-03 2.2888184e-05 3.0237436e-04] \n",
      "\n",
      "[0.34000444 0.11846979 0.0532501 ] \n",
      " [0.3391239  0.11950401 0.05366477] \n",
      " [0.00088054 0.00103422 0.00041466] \n",
      "\n",
      "[0.2787743  0.09795745 0.04382458] \n",
      " [0.27820867 0.09803806 0.04402522] \n",
      " [5.6561828e-04 8.0615282e-05 2.0063296e-04] \n",
      "\n",
      "[0.38783568 0.13837242 0.06166521] \n",
      " [0.38505805 0.13569076 0.06093363] \n",
      " [0.00277764 0.00268166 0.00073158] \n",
      "\n",
      "[0.6198237  0.2168765  0.09834416] \n",
      " [0.62874156 0.22156247 0.0994954 ] \n",
      " [0.00891787 0.00468597 0.00115124] \n",
      "\n",
      "[0.11169361 0.04356504 0.01761535] \n",
      " [0.10489522 0.03696407 0.01659918] \n",
      " [0.00679839 0.00660098 0.00101618] \n",
      "\n",
      "[0.54214245 0.1917189  0.08476359] \n",
      " [0.53214633 0.18752325 0.08420966] \n",
      " [0.00999612 0.00419566 0.00055393] \n",
      "\n",
      "[0.0656703  0.14371641 0.00844482] \n",
      " [0.06156887 0.22126742 0.00974299] \n",
      " [0.00410143 0.07755101 0.00129817] \n",
      "\n",
      "[0.1906498  0.0634458  0.03138269] \n",
      " [0.18743345 0.06604975 0.02966047] \n",
      " [0.00321634 0.00260395 0.00172223] \n",
      "\n",
      "[0.10544837 0.03092439 0.01627876] \n",
      " [0.10765368 0.03793612 0.01703569] \n",
      " [0.0022053  0.00701174 0.00075693] \n",
      "\n",
      "[0.60422516 0.21232323 0.09564332] \n",
      " [0.61361426 0.21623178 0.09710158] \n",
      " [0.0093891  0.00390854 0.00145826] \n",
      "\n",
      "[0.049249   0.0629628  0.00811338] \n",
      " [0.05984951 0.06631313 0.00947091] \n",
      " [0.01060051 0.00335033 0.00135753] \n",
      "\n",
      "[0.48229277 0.17092136 0.07623157] \n",
      " [0.48086026 0.16945055 0.07609389] \n",
      " [0.00143251 0.0014708  0.00013768] \n",
      "\n",
      "[0.06203742 0.09411    0.01071992] \n",
      " [0.07409985 0.10988183 0.01172596] \n",
      " [0.01206244 0.01577183 0.00100603] \n",
      "\n",
      "[0.4661646  0.16215663 0.0727507 ] \n",
      " [0.45858058 0.16159938 0.07256822] \n",
      " [0.00758401 0.00055724 0.00018248] \n",
      "\n",
      "[0.45541412 0.16051221 0.07125271] \n",
      " [0.45056218 0.15877378 0.07129935] \n",
      " [4.8519373e-03 1.7384291e-03 4.6640635e-05] \n",
      "\n",
      "[0.46084344 0.1617088  0.07217194] \n",
      " [0.45381433 0.15991981 0.07181399] \n",
      " [0.00702912 0.00178899 0.00035796] \n",
      "\n",
      "[0.4014461  0.14509694 0.06431877] \n",
      " [0.40157783 0.14151217 0.06354781] \n",
      " [0.00013173 0.00358477 0.00077096] \n",
      "\n",
      "[0.6114284  0.21544775 0.09720957] \n",
      " [0.61976445 0.21839905 0.09807482] \n",
      " [0.00833607 0.00295129 0.00086524] \n",
      "\n",
      "[0.37691104 0.1328288  0.0587187 ] \n",
      " [0.36575904 0.12888998 0.05787965] \n",
      " [0.011152   0.00393882 0.00083904] \n",
      "\n",
      "[0.17132843 0.06141352 0.02749111] \n",
      " [0.16941184 0.05969911 0.02680863] \n",
      " [0.00191659 0.00171441 0.00068248] \n",
      "\n",
      "[0.33487868 0.12203119 0.05394537] \n",
      " [0.33283466 0.11728773 0.05266953] \n",
      " [0.00204402 0.00474346 0.00127585] \n",
      "\n",
      "[0.06368184 0.15777104 0.00900285] \n",
      " [0.05099608 0.16424087 0.00806989] \n",
      " [0.01268576 0.00646983 0.00093296] \n",
      "\n",
      "[0.21202391 0.04476318 0.0396064 ] \n",
      " [0.20912723 0.07369442 0.0330934 ] \n",
      " [0.00289668 0.02893125 0.006513  ] \n",
      "\n",
      "[0.5735159  0.20275393 0.09021897] \n",
      " [0.5636969  0.19864139 0.0892024 ] \n",
      " [0.00981897 0.00411254 0.00101657] \n",
      "\n",
      "[0.55455256 0.19517879 0.08736946] \n",
      " [0.5423201  0.19110838 0.08581961] \n",
      " [0.01223248 0.00407042 0.00154985] \n",
      "\n",
      "[0.20984592 0.07326914 0.03289378] \n",
      " [0.21466339 0.07564531 0.03396948] \n",
      " [0.00481747 0.00237617 0.00107569] \n",
      "\n",
      "[0.10814537 0.03031333 0.01629836] \n",
      " [0.10919179 0.03847814 0.01727909] \n",
      " [0.00104642 0.00816481 0.00098073] \n",
      "\n",
      "[0.59793    0.20998773 0.09402094] \n",
      " [0.5905543  0.20810565 0.09345245] \n",
      " [0.00737572 0.00188208 0.00056849] \n",
      "\n",
      "[0.19173585 0.06395798 0.03124292] \n",
      " [0.1928206  0.06794813 0.03051295] \n",
      " [0.00108474 0.00399014 0.00072997] \n",
      "\n",
      "[0.04877859 0.1045989  0.00764373] \n",
      " [0.04785309 0.10926305 0.00757253] \n",
      " [9.2550740e-04 4.6641454e-03 7.1198680e-05] \n",
      "\n",
      "[0.51659757 0.18211193 0.08097965] \n",
      " [0.50806046 0.17903563 0.08039819] \n",
      " [0.00853711 0.0030763  0.00058146] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.06702003 0.1315546  0.01115726] \n",
      " [0.08031958 0.04709388 0.0127102 ] \n",
      " [0.01329955 0.08446072 0.00155294] \n",
      "\n",
      "[0.0704561  0.02265989 0.01251825] \n",
      " [0.08031547 0.01313556 0.01270955] \n",
      " [0.00985938 0.00952433 0.0001913 ] \n",
      "\n",
      "[0.5900985  0.20723471 0.09288756] \n",
      " [0.58325016 0.20553175 0.0922966 ] \n",
      " [0.00684834 0.00170296 0.00059096] \n",
      "\n",
      "[0.33295172 0.12290361 0.06339688] \n",
      " [0.20922175 0.07372773 0.03310836] \n",
      " [0.12372997 0.04917587 0.03028852] \n",
      "\n",
      "[0.40410817 0.13750425 0.06164438] \n",
      " [0.40197957 0.14165375 0.06361138] \n",
      " [0.0021286 0.0041495 0.001967 ] \n",
      "\n",
      "[0.16750419 0.05602016 0.02731135] \n",
      " [0.16985492 0.05985525 0.02687875] \n",
      " [0.00235073 0.00383509 0.0004326 ] \n",
      "\n",
      "[0.5979062 0.2111435 0.094464 ] \n",
      " [0.5925186  0.20879786 0.09376329] \n",
      " [0.00538754 0.00234564 0.0007007 ] \n",
      "\n",
      "[0.47850913 0.1692249  0.07555073] \n",
      " [0.44950888 0.1584026  0.07113267] \n",
      " [0.02900025 0.0108223  0.00441806] \n",
      "\n",
      "[0.22879285 0.08868051 0.03696114] \n",
      " [0.23121746 0.08147881 0.03658908] \n",
      " [0.00242461 0.0072017  0.00037206] \n",
      "\n",
      "[0.12737828 0.04565632 0.01888561] \n",
      " [0.12284148 0.04328816 0.01943909] \n",
      " [0.00453681 0.00236816 0.00055348] \n",
      "\n",
      "[0.5928215  0.20833197 0.09342333] \n",
      " [0.5879452  0.20718625 0.09303958] \n",
      " [0.00487626 0.00114572 0.00038375] \n",
      "\n",
      "[0.05550975 0.20487773 0.00904182] \n",
      " [0.06122897 0.20311797 0.0096892 ] \n",
      " [0.00571922 0.00175977 0.00064737] \n",
      "\n",
      "[0.4563474  0.159868   0.07143731] \n",
      " [0.4473052  0.15762606 0.07078395] \n",
      " [0.0090422  0.00224194 0.00065336] \n",
      "\n",
      "[0.4991815  0.17612803 0.07834075] \n",
      " [0.47793642 0.1684202  0.07563119] \n",
      " [0.02124509 0.00770783 0.00270955] \n",
      "\n",
      "[0.6089477  0.21363778 0.0963792 ] \n",
      " [0.6166575  0.21730419 0.09758316] \n",
      " [0.0077098  0.0036664  0.00120396] \n",
      "\n",
      "[0.5224808  0.18547821 0.08203282] \n",
      " [0.51979107 0.18316938 0.08225449] \n",
      " [0.00268972 0.00230883 0.00022167] \n",
      "\n",
      "[0.30632484 0.10883107 0.04760623] \n",
      " [0.3086335  0.10875949 0.0488398 ] \n",
      " [2.3086667e-03 7.1585178e-05 1.2335777e-03] \n",
      "\n",
      "[0.25054958 0.08910304 0.03885411] \n",
      " [0.24644502 0.08684486 0.03899877] \n",
      " [0.00410457 0.00225817 0.00014465] \n",
      "\n",
      "[0.16150872 0.05057259 0.02263376] \n",
      " [0.14959018 0.05271415 0.02367195] \n",
      " [0.01191854 0.00214155 0.00103819] \n",
      "\n",
      "[0.33091253 0.11194067 0.05165752] \n",
      " [0.32810304 0.11562035 0.05192076] \n",
      " [0.00280949 0.00367969 0.00026324] \n",
      "\n",
      "[0.25674856 0.08858985 0.03931409] \n",
      " [0.24642453 0.08683763 0.03899553] \n",
      " [0.01032403 0.00175222 0.00031856] \n",
      "\n",
      "[0.4859888  0.17109756 0.07581599] \n",
      " [0.48194242 0.16983189 0.07626513] \n",
      " [0.00404638 0.00126567 0.00044914] \n",
      "\n",
      "[0.51408046 0.18069461 0.08040028] \n",
      " [0.5024314  0.17705199 0.07950741] \n",
      " [0.01164907 0.00364262 0.00089287] \n",
      "\n",
      "[0.17489626 0.06361227 0.024445  ] \n",
      " [0.17938858 0.06321482 0.0283874 ] \n",
      " [0.00449233 0.00039745 0.00394241] \n",
      "\n",
      "[0.12030958 0.02710164 0.01884187] \n",
      " [0.13647594 0.0480928  0.02159668] \n",
      " [0.01616635 0.02099117 0.00275481] \n",
      "\n",
      "[0.50323254 0.17812403 0.07850502] \n",
      " [0.493492   0.17390184 0.07809279] \n",
      " [0.00974053 0.00422218 0.00041223] \n",
      "\n",
      "[0.12570694 0.05133494 0.01887147] \n",
      " [0.11799673 0.04158092 0.01867243] \n",
      " [0.00771021 0.00975402 0.00019904] \n",
      "\n",
      "[0.03507622 0.14444813 0.00515119] \n",
      " [0.0484235  0.21406989 0.00766279] \n",
      " [0.01334728 0.06962176 0.00251161] \n",
      "\n",
      "[0.29970384 0.10075037 0.0436173 ] \n",
      " [0.3049727  0.10746945 0.0482605 ] \n",
      " [0.00526887 0.00671908 0.0046432 ] \n",
      "\n",
      "[0.03654112 0.1551665  0.0053481 ] \n",
      " [0.04773386 0.15089168 0.00755366] \n",
      " [0.01119274 0.00427483 0.00220556] \n",
      "\n",
      "[0.04966198 0.10510087 0.00730844] \n",
      " [0.06419071 0.08175176 0.01015788] \n",
      " [0.01452873 0.02334911 0.00284943] \n",
      "\n",
      "[0.41652858 0.146341   0.06539293] \n",
      " [0.41635418 0.14671922 0.0658861 ] \n",
      " [0.0001744  0.00037822 0.00049318] \n",
      "\n",
      "[0.1897335  0.06434605 0.02954224] \n",
      " [0.1820262  0.06414429 0.0288048 ] \n",
      " [0.0077073  0.00020175 0.00073745] \n",
      "\n",
      "[0.23648216 0.091507   0.04059302] \n",
      " [0.24907681 0.08777227 0.03941524] \n",
      " [0.01259466 0.00373473 0.00117778] \n",
      "\n",
      "[0.21544214 0.07093639 0.03265698] \n",
      " [0.21630408 0.07622348 0.03422911] \n",
      " [0.00086194 0.00528709 0.00157213] \n",
      "\n",
      "[0.44655424 0.16039369 0.06970746] \n",
      " [0.43636224 0.15376987 0.06905228] \n",
      " [0.01019201 0.00662382 0.00065518] \n",
      "\n",
      "[0.15473643 0.04656982 0.02663376] \n",
      " [0.16556534 0.05834364 0.02619994] \n",
      " [0.01082891 0.01177382 0.00043382] \n",
      "\n",
      "[0.09522011 0.09246676 0.01704028] \n",
      " [0.11938839 0.04207132 0.01889265] \n",
      " [0.02416828 0.05039544 0.00185237] \n",
      "\n",
      "[0.4550956  0.16112456 0.07188464] \n",
      " [0.4501771  0.15863809 0.07123841] \n",
      " [0.00491849 0.00248647 0.00064623] \n",
      "\n",
      "[0.34434247 0.11833081 0.05370584] \n",
      " [0.33622733 0.11848328 0.0532064 ] \n",
      " [0.00811514 0.00015247 0.00049944] \n",
      "\n",
      "[0.2862129  0.1001658  0.04442663] \n",
      " [0.28241017 0.09951863 0.04469009] \n",
      " [0.00380272 0.00064716 0.00026345] \n",
      "\n",
      "[0.39841443 0.13589463 0.06102625] \n",
      " [0.39543748 0.13934836 0.06257612] \n",
      " [0.00297695 0.00345373 0.00154987] \n",
      "\n",
      "[0.22320822 0.07891545 0.03439457] \n",
      " [0.22589698 0.07960393 0.03574714] \n",
      " [0.00268877 0.00068848 0.00135257] \n",
      "\n",
      "[0.27065033 0.0975038  0.0434631 ] \n",
      " [0.2679662  0.09442871 0.04240439] \n",
      " [0.00268412 0.00307509 0.00105871] \n",
      "\n",
      "[0.60399896 0.21391582 0.09502795] \n",
      " [0.6038978 0.2128078 0.095564 ] \n",
      " [0.00010115 0.00110802 0.00053605] \n",
      "\n",
      "[0.37475258 0.13030688 0.05782315] \n",
      " [0.36520526 0.12869483 0.05779202] \n",
      " [9.5473230e-03 1.6120523e-03 3.1128526e-05] \n",
      "\n",
      "[0.29909176 0.10553248 0.04678496] \n",
      " [0.2933087  0.10335916 0.04641473] \n",
      " [0.00578305 0.00217332 0.00037023] \n",
      "\n",
      "[0.2036321  0.06792797 0.0336246 ] \n",
      " [0.19715276 0.06947474 0.0311985 ] \n",
      " [0.00647934 0.00154677 0.0024261 ] \n",
      "\n",
      "[0.1449995  0.05155687 0.02268379] \n",
      " [0.12895864 0.04544378 0.0204071 ] \n",
      " [0.01604086 0.00611308 0.00227669] \n",
      "\n",
      "[0.08550231 0.03823122 0.01205206] \n",
      " [0.08156702 0.03652159 0.0129076 ] \n",
      " [0.00393529 0.00170963 0.00085554] \n",
      "\n",
      "[0.43807697 0.15723419 0.06920099] \n",
      " [0.43972242 0.15495396 0.06958401] \n",
      " [0.00164545 0.00228024 0.00038303] \n",
      "\n",
      "[0.3043896  0.09920396 0.0447766 ] \n",
      " [0.3017857  0.10634638 0.04775617] \n",
      " [0.00260389 0.00714242 0.00297957] \n",
      "\n",
      "[0.5672356  0.20065697 0.08867396] \n",
      " [0.56004006 0.19735274 0.08862372] \n",
      " [7.1955323e-03 3.3042282e-03 5.0246716e-05] \n",
      "\n",
      "[0.5118185  0.18095808 0.08074077] \n",
      " [0.48348212 0.17037445 0.07650878] \n",
      " [0.02833641 0.01058362 0.00423199] \n",
      "\n",
      "[0.5773709  0.20448795 0.09090818] \n",
      " [0.56971604 0.20076247 0.09015489] \n",
      " [0.00765485 0.00372548 0.00075329] \n",
      "\n",
      "[0.3520934  0.1240361  0.05573658] \n",
      " [0.34396598 0.12121031 0.054431  ] \n",
      " [0.00812742 0.00282578 0.00130558] \n",
      "\n",
      "[0.07219699 0.17937572 0.01022707] \n",
      " [0.04891202 0.1877023  0.0077401 ] \n",
      " [0.02328497 0.00832658 0.00248698] \n",
      "\n",
      "[0.3385015  0.12058859 0.05095388] \n",
      " [0.3458999  0.12189181 0.05473704] \n",
      " [0.0073984  0.00130323 0.00378316] \n",
      "\n",
      "[0.21855114 0.07698157 0.04012814] \n",
      " [0.21537891 0.07589746 0.0340827 ] \n",
      " [0.00317223 0.00108411 0.00604543] \n",
      "\n",
      "[0.27415168 0.09782347 0.0440498 ] \n",
      " [0.26977178 0.09506499 0.04269012] \n",
      " [0.0043799  0.00275848 0.00135968] \n",
      "\n",
      "[0.17305043 0.05930665 0.02646487] \n",
      " [0.16837989 0.05933546 0.02664533] \n",
      " [4.6705455e-03 2.8807670e-05 1.8045679e-04] \n",
      "\n",
      "[0.3437804  0.12108128 0.05441615] \n",
      " [0.34068885 0.12005547 0.05391241] \n",
      " [0.00309154 0.0010258  0.00050374] \n",
      "\n",
      "[0.61948365 0.2168395  0.09809526] \n",
      " [0.6242898  0.21999374 0.09879094] \n",
      " [0.00480616 0.00315423 0.00069568] \n",
      "\n",
      "[0.43219173 0.15588968 0.06827789] \n",
      " [0.43727484 0.15409145 0.06919669] \n",
      " [0.00508311 0.00179823 0.00091881] \n",
      "\n",
      "[0.02971997 0.06531841 0.00400507] \n",
      " [0.01055877 0.04852362 0.00167088] \n",
      " [0.0191612  0.0167948  0.00233419] \n",
      "\n",
      "[0.6071455  0.21358661 0.09562684] \n",
      " [0.6031429  0.21254177 0.09544454] \n",
      " [0.00400257 0.00104484 0.0001823 ] \n",
      "\n",
      "[0.18238221 0.06343719 0.02863591] \n",
      " [0.1841068  0.06487747 0.02913404] \n",
      " [0.00172459 0.00144027 0.00049813] \n",
      "\n",
      "[0.17713763 0.06452232 0.02743038] \n",
      " [0.17780481 0.06265671 0.02813678] \n",
      " [0.00066718 0.00186561 0.0007064 ] \n",
      "\n",
      "[0.10912619 0.03197557 0.01717246] \n",
      " [0.10983872 0.03870611 0.01738146] \n",
      " [0.00071253 0.00673053 0.00020901] \n",
      "\n",
      "[0.3823734  0.13326776 0.05929216] \n",
      " [0.3799697  0.13389768 0.06012842] \n",
      " [0.00240371 0.00062992 0.00083626] \n",
      "\n",
      "[0.033548   0.1802818  0.00533137] \n",
      " [0.05005892 0.17955923 0.00792159] \n",
      " [0.01651092 0.00072257 0.00259022] \n",
      "\n",
      "[0.46289074 0.16079228 0.07208791] \n",
      " [0.4537109  0.15988338 0.07179762] \n",
      " [0.00917983 0.0009089  0.00029028] \n",
      "\n",
      "[0.5667007  0.2005374  0.08858895] \n",
      " [0.5597153  0.19723828 0.08857232] \n",
      " [6.9854259e-03 3.2991171e-03 1.6629696e-05] \n",
      "\n",
      "[0.07074123 0.02155978 0.01098341] \n",
      " [0.07046784 0.01265552 0.0111512 ] \n",
      " [0.00027339 0.00890426 0.0001678 ] \n",
      "\n",
      "[0.2786572  0.09513482 0.0425409 ] \n",
      " [0.27553037 0.09709426 0.04360139] \n",
      " [0.00312683 0.00195944 0.00106049] \n",
      "\n",
      "[0.5955828  0.2094523  0.09388521] \n",
      " [0.5834006  0.20558476 0.09232041] \n",
      " [0.01218218 0.00386754 0.00156479] \n",
      "\n",
      "[0.03409155 0.109394   0.00360323] \n",
      " [0.01804509 0.10782219 0.00285555] \n",
      " [0.01604645 0.0015718  0.00074767] \n",
      "\n",
      "[0.45445907 0.15975201 0.07133048] \n",
      " [0.44761395 0.15773486 0.07083281] \n",
      " [0.00684512 0.00201716 0.00049767] \n",
      "\n",
      "[0.04488938 0.0344899  0.00571223] \n",
      " [0.03673641 0.0128104  0.00581337] \n",
      " [0.00815297 0.0216795  0.00010113] \n",
      "\n",
      "[0.17201523 0.05625119 0.02689815] \n",
      " [0.16987216 0.05986132 0.02688148] \n",
      " [2.1430701e-03 3.6101341e-03 1.6678125e-05] \n",
      "\n",
      "[0.3146652  0.10623777 0.04766246] \n",
      " [0.33028397 0.1163889  0.05226589] \n",
      " [0.01561877 0.01015113 0.00460344] \n",
      "\n",
      "[0.60370153 0.21140218 0.09584448] \n",
      " [0.60813326 0.2143003  0.09623423] \n",
      " [0.00443172 0.00289813 0.00038975] \n",
      "\n",
      "[0.4306152  0.14749184 0.06637973] \n",
      " [0.4214498  0.14851487 0.06669246] \n",
      " [0.00916538 0.00102302 0.00031273] \n",
      "\n",
      "[0.04034675 0.04425847 0.00678675] \n",
      " [0.02470497 0.02471112 0.00390944] \n",
      " [0.01564178 0.01954735 0.00287731] \n",
      "\n",
      "[0.41088617 0.1421045  0.06407116] \n",
      " [0.4070381  0.14343631 0.06441187] \n",
      " [0.00384808 0.00133181 0.00034071] \n",
      "\n",
      "[0.02287099 0.1631294  0.0027967 ] \n",
      " [0.00518504 0.22056611 0.00082051] \n",
      " [0.01768595 0.0574367  0.00197619] \n",
      "\n",
      "[0.04339886 0.0394132  0.00771058] \n",
      " [0.0319209  0.03093056 0.00505133] \n",
      " [0.01147796 0.00848264 0.00265925] \n",
      "\n",
      "[0.32895482 0.11150078 0.05214415] \n",
      " [0.32005852 0.11278556 0.05064777] \n",
      " [0.00889629 0.00128477 0.00149639] \n",
      "\n",
      "[0.4804396  0.16876402 0.07477872] \n",
      " [0.47800344 0.16844383 0.0756418 ] \n",
      " [0.00243616 0.0003202  0.00086308] \n",
      "\n",
      "[0.2252353  0.08034286 0.03598433] \n",
      " [0.22311762 0.0786245  0.03530732] \n",
      " [0.00211768 0.00171836 0.00067701] \n",
      "\n",
      "[0.47756016 0.16759363 0.07461504] \n",
      " [0.47381815 0.16696897 0.0749795 ] \n",
      " [0.00374201 0.00062466 0.00036446] \n",
      "\n",
      "[0.16567041 0.06239646 0.02766876] \n",
      " [0.16118328 0.05679944 0.0255065 ] \n",
      " [0.00448713 0.00559702 0.00216226] \n",
      "\n",
      "[0.33710438 0.1155564  0.05227741] \n",
      " [0.3318282  0.11693307 0.05251026] \n",
      " [0.00527617 0.00137667 0.00023285] \n",
      "\n",
      "[0.2702263  0.10258608 0.04550764] \n",
      " [0.27253696 0.09603941 0.0431277 ] \n",
      " [0.00231066 0.00654666 0.00237995] \n",
      "\n",
      "[0.0517405  0.06678743 0.0074497 ] \n",
      " [0.05206164 0.06953268 0.00823851] \n",
      " [0.00032115 0.00274525 0.00078881] \n",
      "\n",
      "[0.06943166 0.16307704 0.01140229] \n",
      " [0.07816765 0.15305026 0.01236966] \n",
      " [0.00873599 0.01002678 0.00096737] \n",
      "\n",
      "[0.27233896 0.09882812 0.04418186] \n",
      " [0.2695772  0.09499642 0.04265933] \n",
      " [0.00276175 0.0038317  0.00152253] \n",
      "\n",
      "[0.5577076  0.19615188 0.08803928] \n",
      " [0.5457917  0.19233176 0.08636898] \n",
      " [0.01191592 0.00382012 0.0016703 ] \n",
      "\n",
      "[0.5002966  0.17470327 0.07759505] \n",
      " [0.49380833 0.17401332 0.07814285] \n",
      " [0.00648826 0.00068995 0.0005478 ] \n",
      "\n",
      "[0.37176025 0.12712368 0.05830483] \n",
      " [0.37313876 0.13149051 0.05904746] \n",
      " [0.00137851 0.00436683 0.00074263] \n",
      "\n",
      "[0.3820632  0.13451894 0.06065935] \n",
      " [0.38396716 0.13530633 0.060761  ] \n",
      " [0.00190395 0.00078739 0.00010166] \n",
      "\n",
      "[0.2576021  0.09279985 0.04221878] \n",
      " [0.27835557 0.09808983 0.04404847] \n",
      " [0.02075347 0.00528998 0.00182968] \n",
      "\n",
      "[0.26533705 0.09330456 0.0410029 ] \n",
      " [0.25554448 0.09005142 0.04043872] \n",
      " [0.00979257 0.00325314 0.00056418] \n",
      "\n",
      "[0.5138626  0.1798757  0.08037005] \n",
      " [0.485403   0.17105135 0.07681275] \n",
      " [0.02845961 0.00882435 0.0035573 ] \n",
      "\n",
      "[0.3915438 0.1380867 0.0602234] \n",
      " [0.38864693 0.13695545 0.06150155] \n",
      " [0.00289688 0.00113125 0.00127815] \n",
      "\n",
      "[0.28914714 0.10204756 0.04617409] \n",
      " [0.28984347 0.10213806 0.04586637] \n",
      " [6.9633126e-04 9.0494752e-05 3.0772015e-04] \n",
      "\n",
      "[0.44384134 0.15680893 0.07025417] \n",
      " [0.43946615 0.15486366 0.06954346] \n",
      " [0.00437519 0.00194527 0.00071071] \n",
      "\n",
      "[0.08915268 0.12241104 0.01460211] \n",
      " [0.09270424 0.19657192 0.01467001] \n",
      " [3.5515577e-03 7.4160881e-02 6.7897141e-05] \n",
      "\n",
      "[0.55532974 0.19770232 0.0874451 ] \n",
      " [0.53330946 0.18793313 0.08439372] \n",
      " [0.02202028 0.00976919 0.00305138] \n",
      "\n",
      "[0.5189347  0.18255961 0.08135568] \n",
      " [0.50706273 0.17868404 0.0802403 ] \n",
      " [0.01187199 0.00387557 0.00111538] \n",
      "\n",
      "[0.13853523 0.05156186 0.0227098 ] \n",
      " [0.13709149 0.04830972 0.02169408] \n",
      " [0.00144374 0.00325214 0.00101572] \n",
      "\n",
      "[0.31608886 0.11190472 0.05033967] \n",
      " [0.31954098 0.11260317 0.05056586] \n",
      " [0.00345212 0.00069845 0.0002262 ] \n",
      "\n",
      "[0.6181844  0.21729025 0.09809725] \n",
      " [0.6318179  0.22264655 0.09998222] \n",
      " [0.01363349 0.0053563  0.00188497] \n",
      "\n",
      "[0.03799546 0.11034052 0.00259055] \n",
      " [0.00375201 0.11252202 0.00059374] \n",
      " [0.03424345 0.0021815  0.00199682] \n",
      "\n",
      "[0.26877803 0.09365669 0.04527627] \n",
      " [0.264558   0.09322769 0.04186506] \n",
      " [0.00422004 0.000429   0.00341121] \n",
      "\n",
      "[0.5679921  0.20055088 0.08921172] \n",
      " [0.5612515  0.19777966 0.08881542] \n",
      " [0.00674057 0.00277123 0.0003963 ] \n",
      "\n",
      "[0.03979439 0.08729926 0.00667129] \n",
      " [0.05868485 0.0970118  0.0092866 ] \n",
      " [0.01889046 0.00971255 0.00261531] \n",
      "\n",
      "[0.5774722  0.20225616 0.09090742] \n",
      " [0.5774617  0.20349197 0.09138061] \n",
      " [1.04904175e-05 1.23581290e-03 4.73193824e-04] \n",
      "\n",
      "[0.14884147 0.05763448 0.0263383 ] \n",
      " [0.1557364  0.05488001 0.02464456] \n",
      " [0.00689493 0.00275447 0.00169374] \n",
      "\n",
      "[0.38444984 0.13428931 0.05913248] \n",
      " [0.37904924 0.13357332 0.05998277] \n",
      " [0.0054006  0.00071599 0.00085028] \n",
      "\n",
      "[0.25999933 0.09173454 0.04083921] \n",
      " [0.26694283 0.09406809 0.04224245] \n",
      " [0.00694349 0.00233354 0.00140323] \n",
      "\n",
      "[0.4494825  0.15649645 0.06969467] \n",
      " [0.4451115  0.15685302 0.07043681] \n",
      " [0.00437099 0.00035657 0.00074214] \n",
      "\n",
      "[0.25943643 0.09911065 0.03610336] \n",
      " [0.25567308 0.09009674 0.04045907] \n",
      " [0.00376335 0.00901391 0.0043557 ] \n",
      "\n",
      "[0.37131882 0.13178797 0.05705696] \n",
      " [0.36828145 0.12977885 0.05827881] \n",
      " [0.00303736 0.00200912 0.00122185] \n",
      "\n",
      "[0.099231   0.0936728  0.01827274] \n",
      " [0.12539555 0.04418819 0.01984326] \n",
      " [0.02616455 0.04948462 0.00157051] \n",
      "\n",
      "[0.58905786 0.20675609 0.09225804] \n",
      " [0.5746392  0.20249733 0.09093396] \n",
      " [0.01441866 0.00425875 0.00132409] \n",
      "\n",
      "[0.24598859 0.0944844  0.03043567] \n",
      " [0.19741124 0.06956583 0.0312394 ] \n",
      " [0.04857735 0.02491858 0.00080374] \n",
      "\n",
      "[0.58841914 0.2080956  0.09251975] \n",
      " [0.5903526  0.20803459 0.09342053] \n",
      " [1.9334555e-03 6.1005354e-05 9.0078264e-04] \n",
      "\n",
      "[0.30455488 0.10662158 0.04740833] \n",
      " [0.3022754  0.10651895 0.04783366] \n",
      " [0.00227949 0.00010263 0.00042533] \n",
      "\n",
      "[0.40120393 0.14493898 0.06378047] \n",
      " [0.40194464 0.14164142 0.06360585] \n",
      " [0.00074071 0.00329755 0.00017462] \n",
      "\n",
      "[0.5386226  0.19132403 0.08478956] \n",
      " [0.5289087  0.18638235 0.08369733] \n",
      " [0.00971389 0.00494167 0.00109223] \n",
      "\n",
      "[0.40677416 0.13891649 0.06268274] \n",
      " [0.40334365 0.14213443 0.06382725] \n",
      " [0.00343052 0.00321794 0.00114451] \n",
      "\n",
      "[0.1701347  0.06047852 0.02626543] \n",
      " [0.1770559  0.06239279 0.02801827] \n",
      " [0.0069212  0.00191428 0.00175284] \n",
      "\n",
      "[0.33526945 0.11936543 0.053097  ] \n",
      " [0.337416   0.11890215 0.0533945 ] \n",
      " [0.00214654 0.00046328 0.0002975 ] \n",
      "\n",
      "[0.34251028 0.1174484  0.05478722] \n",
      " [0.34898013 0.12297724 0.05522447] \n",
      " [0.00646985 0.00552884 0.00043725] \n",
      "\n",
      "[0.10819781 0.04671644 0.01631442] \n",
      " [0.11643614 0.04103098 0.01842547] \n",
      " [0.00823833 0.00568546 0.00211105] \n",
      "\n",
      "[0.59371966 0.21010692 0.09408709] \n",
      " [0.597088   0.21040806 0.09448637] \n",
      " [0.00336832 0.00030114 0.00039928] \n",
      "\n",
      "[0.38381243 0.13742796 0.06043696] \n",
      " [0.3807621  0.13417691 0.06025382] \n",
      " [0.00305033 0.00325105 0.00018314] \n",
      "\n",
      "[0.61198974 0.21482055 0.09682678] \n",
      " [0.6147477  0.2166312  0.09728094] \n",
      " [0.00275797 0.00181065 0.00045417] \n",
      "\n",
      "[0.60545754 0.21229711 0.09526597] \n",
      " [0.59929496 0.21118578 0.09483562] \n",
      " [0.00616258 0.00111133 0.00043035] \n",
      "\n",
      "[0.09646055 0.03532932 0.01397275] \n",
      " [0.10015348 0.03529312 0.01584882] \n",
      " [3.6929324e-03 3.6198646e-05 1.8760711e-03] \n",
      "\n",
      "[0.30410588 0.10695148 0.04717359] \n",
      " [0.30088654 0.10602953 0.04761389] \n",
      " [0.00321934 0.00092194 0.0004403 ] \n",
      "\n",
      "[0.04047789 0.06956898 0.00619583] \n",
      " [0.03354742 0.06591654 0.00530872] \n",
      " [0.00693047 0.00365245 0.00088711] \n",
      "\n",
      "[0.0356596  0.18403573 0.00706329] \n",
      " [0.03784492 0.20780043 0.00598878] \n",
      " [0.00218532 0.0237647  0.0010745 ] \n",
      "\n",
      "[0.60570025 0.21465178 0.0958943 ] \n",
      " [0.6231569  0.21959451 0.09861165] \n",
      " [0.01745665 0.00494273 0.00271735] \n",
      "\n",
      "[0.2710509  0.09522763 0.04321326] \n",
      " [0.27258626 0.09605677 0.04313549] \n",
      " [1.535356e-03 8.291453e-04 7.776171e-05] \n",
      "\n",
      "[0.17913468 0.0652424  0.03045756] \n",
      " [0.15912214 0.05607312 0.02518033] \n",
      " [0.02001254 0.00916928 0.00527723] \n",
      "\n",
      "[0.1252968  0.04528879 0.0196631 ] \n",
      " [0.11902619 0.04194369 0.01883534] \n",
      " [0.00627061 0.00334511 0.00082777] \n",
      "\n",
      "[0.42077297 0.14696935 0.06538685] \n",
      " [0.41526583 0.14633569 0.06571387] \n",
      " [0.00550714 0.00063366 0.00032702] \n",
      "\n",
      "[0.47729927 0.16752288 0.07463196] \n",
      " [0.47340763 0.16682431 0.07491454] \n",
      " [0.00389165 0.00069857 0.00028258] \n",
      "\n",
      "[0.07453191 0.18409495 0.01398471] \n",
      " [0.08886633 0.18627122 0.01406268] \n",
      " [1.4334418e-02 2.1762699e-03 7.7970326e-05] \n",
      "\n",
      "[0.19659752 0.06723204 0.03704525] \n",
      " [0.20945002 0.07380818 0.03314448] \n",
      " [0.0128525  0.00657614 0.00390077] \n",
      "\n",
      "[0.5010832  0.17726779 0.07857222] \n",
      " [0.4963453  0.17490733 0.07854432] \n",
      " [4.7378838e-03 2.3604631e-03 2.7902424e-05] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmean_dn2 = sum_dn2/300\\nmean_dt2 = sum_dt2/300\\nmean_e2 = sum_e2/300\\nmean_dn2_sq = sum_dn2_sq/300\\nmean_dt2_sq = sum_dt2_sq/300\\nmean_e2_sq = sum_e2_sq/300\\nrelative_dn2 = sum_dn2/sum_dn2_real\\nrelative_dt2 = sum_dt2/sum_dt2_real\\nrelative_e2 = sum_e2/sum_e2_real\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = neural_net.forward(X_validation)\n",
    "pred = pred.data.cpu().numpy()\n",
    "y_val = Y_validation.data.cpu().numpy()\n",
    "print('Predicted\\nReal value\\nDifference\\n')\n",
    "\n",
    "sum_dn1_sq = 0.0\n",
    "sum_dt1_sq = 0.0\n",
    "sum_dn1 = 0.0\n",
    "sum_dt1 = 0.0\n",
    "sum_dn1_real = 0.0\n",
    "sum_dt1_real = 0.0\n",
    "sum_e1 = 0.0\n",
    "sum_e1_sq = 0.0\n",
    "sum_e1_real = 0.0\n",
    "\n",
    "sum_dn2_sq = 0.0\n",
    "sum_dt2_sq = 0.0\n",
    "sum_dn2 = 0.0\n",
    "sum_dt2 = 0.0\n",
    "sum_dn2_real = 0.0\n",
    "sum_dt2_real = 0.0\n",
    "sum_e2 = 0.0\n",
    "sum_e2_sq = 0.0\n",
    "sum_e2_real = 0.0\n",
    "\n",
    "for i in range(300):\n",
    "    # dt1\n",
    "    pred[i][1] /= 2.5\n",
    "    y_val[i][1] /= 2.5\n",
    "    \n",
    "    # e1\n",
    "    pred[i][2] /= 6.3\n",
    "    y_val[i][2] /= 6.3\n",
    "    \n",
    "    # dt2\n",
    "    #pred[i][3] /= 2.5\n",
    "    #y_val[i][3] /= 2.5\n",
    "    \n",
    "    # e2\n",
    "    #pred[i][5] /= 6.3\n",
    "    #y_val[i][5] /= 6.3\n",
    "    \n",
    "    dif = abs(pred[i]-y_val[i])\n",
    "    print(pred[i],'\\n',y_val[i],'\\n',abs(pred[i]-y_val[i]),'\\n')\n",
    "    \n",
    "    sum_dn1_sq += dif[0]**2\n",
    "    sum_dt1_sq += dif[1]**2\n",
    "    sum_e1_sq += dif[2]**2\n",
    "    sum_dn1 += dif[0]\n",
    "    sum_dt1 += dif[1]\n",
    "    sum_e1 += dif[2]\n",
    "    #sum_dn2_sq += dif[2]**2\n",
    "    #sum_dt2_sq += dif[3]**2\n",
    "    #sum_e2_sq += dif[5]**2\n",
    "    #sum_dn2 += dif[2]\n",
    "    #sum_dt2 += dif[3]\n",
    "    #sum_e2 += dif[5]\n",
    "    \n",
    "    sum_dn1_real += y_val[i][0]\n",
    "    sum_dt1_real += y_val[i][1]\n",
    "    sum_e1_real += y_val[i][2]\n",
    "    #sum_dn2_real += y_val[i][2]\n",
    "    #sum_dt2_real += y_val[i][3]\n",
    "    #sum_e2_real += y_val[i][5]\n",
    "    \n",
    "mean_dn1 = sum_dn1/300\n",
    "mean_dt1 = sum_dt1/300\n",
    "mean_e1 = sum_e1/300\n",
    "mean_dn1_sq = sum_dn1_sq/300\n",
    "mean_dt1_sq = sum_dt1_sq/300\n",
    "mean_e1_sq = sum_e1_sq/300\n",
    "relative_dn1 = sum_dn1/sum_dn1_real\n",
    "relative_dt1 = sum_dt1/sum_dt1_real\n",
    "relative_e1 = sum_e1/sum_e1_real\n",
    "\"\"\"\n",
    "mean_dn2 = sum_dn2/300\n",
    "mean_dt2 = sum_dt2/300\n",
    "mean_e2 = sum_e2/300\n",
    "mean_dn2_sq = sum_dn2_sq/300\n",
    "mean_dt2_sq = sum_dt2_sq/300\n",
    "mean_e2_sq = sum_e2_sq/300\n",
    "relative_dn2 = sum_dn2/sum_dn2_real\n",
    "relative_dt2 = sum_dt2/sum_dt2_real\n",
    "relative_e2 = sum_e2/sum_e2_real\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dn1 mean:  0.007990455540517966\n",
      "Dt1 mean:  0.007033813986927271\n",
      "e1 mean:  0.0013129781687166542\n",
      "Dn1 MSE:  0.012975636106926244\n",
      "Dt1 MSE:  0.015964986606097655\n",
      "e1 MSE:  0.002491212326159012\n",
      "Dn1 Relative:  0.026256243587096537\n",
      "Dt1 Relative:  0.05727706870857527\n",
      "e1 Relative:  0.027263882757828657\n"
     ]
    }
   ],
   "source": [
    "print('Dn1 mean: ', mean_dn1)\n",
    "print('Dt1 mean: ', mean_dt1)\n",
    "print('e1 mean: ', mean_e1)\n",
    "#print('Dn2 mean: ', mean_dn2)\n",
    "#print('Dt2 mean: ', mean_dt2)\n",
    "#print('e2 mean: ', mean_e2)\n",
    "print('Dn1 MSE: ', math.sqrt(mean_dn1_sq))\n",
    "print('Dt1 MSE: ', math.sqrt(mean_dt1_sq))\n",
    "print('e1 MSE: ', math.sqrt(mean_e1_sq))\n",
    "#print('Dn2 MSE: ', math.sqrt(mean_dn2_sq))\n",
    "#print('Dt2 MSE: ', math.sqrt(mean_dt2_sq))\n",
    "#print('e2 MSE: ', math.sqrt(mean_e2_sq))\n",
    "print('Dn1 Relative: ', relative_dn1)\n",
    "print('Dt1 Relative: ', relative_dt1)\n",
    "print('e1 Relative: ', relative_e1)\n",
    "#print('Dn2 Relative: ', relative_dn2)\n",
    "#print('Dt2 Relative: ', relative_dt2)\n",
    "#print('e2 Relative: ', relative_e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим как предсказанные значения связаны с аналитической формулой\n",
    "Vs = 2750\n",
    "Vp = 5000\n",
    "g = (Vs**2)/(Vp**2)\n",
    "print('e1 (crack density)\\tDn1 formula\\tPredicted value\\tDifference')\n",
    "sum_dif = 0.0\n",
    "for i in range(300):\n",
    "    e_pred = pred[i][2]\n",
    "    formula = 4*e_pred/(3*g*(1-g))\n",
    "    dn_pred = pred[i][0]\n",
    "    dif = abs(dn_pred-formula)\n",
    "    sum_dif += dif\n",
    "    print(e_pred,'\\t',formula,'\\t',dn_pred,'\\t',dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean difference between predicted value and formula for Dn1: ', sum_dif/300)\n",
    "print('Relative difference between predicted value and formula for Dn1: ', sum_dif/abs(sum_dn1_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим как предсказанные значения связаны с аналитической формулой\n",
    "print('e2 (crack density)\\tDn2 formula\\tPredicted value\\tDifference')\n",
    "sum_dif = 0.0\n",
    "for i in range(300):\n",
    "    e_pred = pred[i][5]\n",
    "    formula = 4*e_pred/(3*g*(1-g))\n",
    "    dn_pred = pred[i][2]\n",
    "    dif = abs(dn_pred-formula)\n",
    "    sum_dif += dif\n",
    "    print(e_pred,'\\t',formula,'\\t',dn_pred,'\\t',dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean difference between predicted value and formula for Dn2: ', sum_dif/300)\n",
    "print('Relative difference between predicted value and formula for Dn2: ', sum_dif/abs(sum_dn2_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('e1 (crack density)\\tDt1 formula\\tPredicted value\\tDifference')\n",
    "sum_dif = 0.0\n",
    "for i in range(300):\n",
    "    e_pred = pred[i][4]\n",
    "    formula = 16*e_pred/(3*(3-2*g))\n",
    "    dt_pred = pred[i][1]\n",
    "    dif = abs(dt_pred-formula)\n",
    "    sum_dif += dif\n",
    "    print(e_pred,'\\t',formula,'\\t',dt_pred,'\\t',dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean difference between predicted value and formula for Dt1: ', sum_dif/300)\n",
    "print('Relative difference between predicted value and formula for Dt1: ', sum_dif/abs(sum_dt1_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('e2 (crack density)\\tDt2 formula\\tPredicted value\\tDifference')\n",
    "sum_dif = 0.0\n",
    "for i in range(300):\n",
    "    e_pred = pred[i][5]\n",
    "    formula = 16*e_pred/(3*(3-2*g))\n",
    "    dt_pred = pred[i][3]\n",
    "    dif = abs(dt_pred-formula)\n",
    "    sum_dif += dif\n",
    "    print(e_pred,'\\t',formula,'\\t',dt_pred,'\\t',dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean difference between predicted value and formula for Dt2: ', sum_dif/300)\n",
    "print('Relative difference between predicted value and formula for Dt2: ', sum_dif/abs(sum_dt_real))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
